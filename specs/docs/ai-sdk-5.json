[
  {
    "title": "AI SDK UI",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui",
    "html": "AI SDK UI\nCopy markdown\nAI SDK UI\nOverview\nGet an overview about the AI SDK UI.\nChatbot\nLearn how to integrate an interface for a chatbot.\nChatbot Message Persistence\nLearn how to store and load chat messages in a chatbot.\nChatbot Tool Usage\nLearn how to integrate an interface for a chatbot with tool calling.\nCompletion\nLearn how to integrate an interface for text completion.\nObject Generation\nLearn how to integrate an interface for object generation.\nStreaming Data\nLearn how to stream data.\nReading UI Message Streams\nLearn how to read UIMessage streams for terminal UIs, custom clients, and server components.\nError Handling\nLearn how to handle errors.\nStream Protocol\nThe stream protocol defines how data is sent from the backend to the AI SDK UI frontend.\nPrevious\nTelemetry\nNext\nOverview"
  },
  {
    "title": "AI SDK UI: Overview",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/overview",
    "html": "AI SDK UI\nOverview\nCopy markdown\nAI SDK UI\n\nAI SDK UI is designed to help you build interactive chat, completion, and assistant applications with ease. It is a framework-agnostic toolkit, streamlining the integration of advanced AI functionalities into your applications.\n\nAI SDK UI provides robust abstractions that simplify the complex tasks of managing chat streams and UI updates on the frontend, enabling you to develop dynamic AI-driven interfaces more efficiently. With three main hooks — useChat, useCompletion, and useObject — you can incorporate real-time chat capabilities, text completions, streamed JSON, and interactive assistant features into your app.\n\nuseChat offers real-time streaming of chat messages, abstracting state management for inputs, messages, loading, and errors, allowing for seamless integration into any UI design.\nuseCompletion enables you to handle text completions in your applications, managing the prompt input and automatically updating the UI as new completions are streamed.\nuseObject is a hook that allows you to consume streamed JSON objects, providing a simple way to handle and display structured data in your application.\n\nThese hooks are designed to reduce the complexity and time required to implement AI interactions, letting you focus on creating exceptional user experiences.\n\nUI Framework Support\n\nAI SDK UI supports the following frameworks: React\n, Svelte\n, Vue.js\n, and Angular\n. Here is a comparison of the supported functions across these frameworks:\n\nFunction\tReact\tSvelte\tVue.js\tAngular\nuseChat\t\n\t\n Chat\t\n\t\n Chat\nuseCompletion\t\n\t\n Completion\t\n\t\n Completion\nuseObject\t\n\t\n StructuredObject\t\n\t\n StructuredObject\n\nContributions\n are welcome to implement missing features for non-React frameworks.\n\nFramework Examples\n\nExplore these example implementations for different frameworks:\n\nNext.js\nNuxt\nSvelteKit\nAngular\nAPI Reference\n\nPlease check out the AI SDK UI API Reference for more details on each function.\n\nPrevious\nAI SDK UI\nNext\nChatbot"
  },
  {
    "title": "AI SDK UI: Chatbot",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot",
    "html": "AI SDK UI\nChatbot\nCopy markdown\nChatbot\n\nThe useChat hook makes it effortless to create a conversational user interface for your chatbot application. It enables the streaming of chat messages from your AI provider, manages the chat state, and updates the UI automatically as new messages arrive.\n\nTo summarize, the useChat hook provides the following features:\n\nMessage Streaming: All the messages from the AI provider are streamed to the chat UI in real-time.\nManaged States: The hook manages the states for input, messages, status, error and more for you.\nSeamless Integration: Easily integrate your chat AI into any design or layout with minimal effort.\n\nIn this guide, you will learn how to use the useChat hook to create a chatbot application with real-time message streaming. Check out our chatbot with tools guide to learn how to use tools in your chatbot. Let's start with the following example first.\n\nExample\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Page() {\n  const { messages, sendMessage, status } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, index) =>\n            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n          )}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          disabled={status !== 'ready'}\n          placeholder=\"Say something...\"\n        />\n        <button type=\"submit\" disabled={status !== 'ready'}>\n          Submit\n        </button>\n      </form>\n    </>\n  );\n}\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    system: 'You are a helpful assistant.',\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nThe UI messages have a new parts property that contains the message parts. We recommend rendering the messages using the parts property instead of the content property. The parts property supports different message types, including text, tool invocation, and tool result, and allows for more flexible and complex chat UIs.\n\nIn the Page component, the useChat hook will request to your AI provider endpoint whenever the user sends a message using sendMessage. The messages are then streamed back in real-time and displayed in the chat UI.\n\nThis enables a seamless chat experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.\n\nCustomized UI\n\nuseChat also provides ways to manage the chat message states via code, show status, and update messages without being triggered by user interactions.\n\nStatus\n\nThe useChat hook returns a status. It has the following possible values:\n\nsubmitted: The message has been sent to the API and we're awaiting the start of the response stream.\nstreaming: The response is actively streaming in from the API, receiving chunks of data.\nready: The full response has been received and processed; a new user message can be submitted.\nerror: An error occurred during the API request, preventing successful completion.\n\nYou can use status for e.g. the following purposes:\n\nTo show a loading spinner while the chatbot is processing the user's message.\nTo show a \"Stop\" button to abort the current message.\nTo disable the submit button.\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Page() {\n  const { messages, sendMessage, status, stop } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <>\n      {messages.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, index) =>\n            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n          )}\n        </div>\n      ))}\n\n\n      {(status === 'submitted' || status === 'streaming') && (\n        <div>\n          {status === 'submitted' && <Spinner />}\n          <button type=\"button\" onClick={() => stop()}>\n            Stop\n          </button>\n        </div>\n      )}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          disabled={status !== 'ready'}\n          placeholder=\"Say something...\"\n        />\n        <button type=\"submit\" disabled={status !== 'ready'}>\n          Submit\n        </button>\n      </form>\n    </>\n  );\n}\nError State\n\nSimilarly, the error state reflects the error object thrown during the fetch request. It can be used to display an error message, disable the submit button, or show a retry button:\n\nWe recommend showing a generic error message to the user, such as \"Something went wrong.\" This is a good practice to avoid leaking information from the server.\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, error, reload } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}:{' '}\n          {m.parts.map((part, index) =>\n            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n          )}\n        </div>\n      ))}\n\n\n      {error && (\n        <>\n          <div>An error occurred.</div>\n          <button type=\"button\" onClick={() => reload()}>\n            Retry\n          </button>\n        </>\n      )}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          disabled={error != null}\n        />\n      </form>\n    </div>\n  );\n}\n\nPlease also see the error handling guide for more information.\n\nModify messages\n\nSometimes, you may want to directly modify some existing messages. For example, a delete button can be added to each message to allow users to remove them from the chat history.\n\nThe setMessages function can help you achieve these tasks:\n\nconst { messages, setMessages } = useChat()\n\n\nconst handleDelete = (id) => {\n  setMessages(messages.filter(message => message.id !== id))\n}\n\n\nreturn <>\n  {messages.map(message => (\n    <div key={message.id}>\n      {message.role === 'user' ? 'User: ' : 'AI: '}\n      {message.parts.map((part, index) => (\n        part.type === 'text' ? (\n          <span key={index}>{part.text}</span>\n        ) : null\n      ))}\n      <button onClick={() => handleDelete(message.id)}>Delete</button>\n    </div>\n  ))}\n  ...\n\nYou can think of messages and setMessages as a pair of state and setState in React.\n\nCancellation and regeneration\n\nIt's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the stop function returned by the useChat hook.\n\nconst { stop, status } = useChat()\n\n\nreturn <>\n  <button onClick={stop} disabled={!(status === 'streaming' || status === 'submitted')}>Stop</button>\n  ...\n\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your chatbot application.\n\nSimilarly, you can also request the AI provider to reprocess the last message by calling the regenerate function returned by the useChat hook:\n\nconst { regenerate, status } = useChat();\n\n\nreturn (\n  <>\n    <button\n      onClick={regenerate}\n      disabled={!(status === 'ready' || status === 'error')}\n    >\n      Regenerate\n    </button>\n    ...\n  </>\n);\n\nWhen the user clicks the \"Regenerate\" button, the AI provider will regenerate the last message and replace the current one correspondingly.\n\nThrottling UI Updates\nThis feature is currently only available for React.\n\nBy default, the useChat hook will trigger a render every time a new chunk is received. You can throttle the UI updates with the experimental_throttle option.\n\npage.tsx\nconst { messages, ... } = useChat({\n  // Throttle the messages and data updates to 50ms:\n  experimental_throttle: 50\n})\nEvent Callbacks\n\nuseChat provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle:\n\nonFinish: Called when the assistant response is completed. The event includes the response message, all messages, and flags for abort, disconnect, and errors.\nonError: Called when an error occurs during the fetch request.\nonData: Called whenever a data part is received.\n\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\nimport { UIMessage } from 'ai';\n\n\nconst {\n  /* ... */\n} = useChat({\n  onFinish: ({ message, messages, isAbort, isDisconnect, isError }) => {\n    // use information to e.g. update other UI states\n  },\n  onError: error => {\n    console.error('An error occurred:', error);\n  },\n  onData: data => {\n    console.log('Received data part from server:', data);\n  },\n});\n\nIt's worth noting that you can abort the processing by throwing an error in the onData callback. This will trigger the onError callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\n\nRequest Configuration\nCustom headers, body, and credentials\n\nBy default, the useChat hook sends a HTTP POST request to the /api/chat endpoint with the message list as the request body. You can customize the request in two ways:\n\nHook-Level Configuration (Applied to all requests)\n\nYou can configure transport-level options that will be applied to all requests made by the hook:\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/custom-chat',\n    headers: {\n      Authorization: 'your_token',\n    },\n    body: {\n      user_id: '123',\n    },\n    credentials: 'same-origin',\n  }),\n});\nDynamic Hook-Level Configuration\n\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/custom-chat',\n    headers: () => ({\n      Authorization: `Bearer ${getAuthToken()}`,\n      'X-User-ID': getCurrentUserId(),\n    }),\n    body: () => ({\n      sessionId: getCurrentSessionId(),\n      preferences: getUserPreferences(),\n    }),\n    credentials: () => 'include',\n  }),\n});\n\nFor component state that changes over time, use useRef to store the current value and reference ref.current in your configuration function, or prefer request-level options (see next section) for better reliability.\n\nRequest-Level Configuration (Recommended)\n\nRecommended: Use request-level options for better flexibility and control. Request-level options take precedence over hook-level options and allow you to customize each request individually.\n\n// Pass options as the second parameter to sendMessage\nsendMessage(\n  { text: input },\n  {\n    headers: {\n      Authorization: 'Bearer token123',\n      'X-Custom-Header': 'custom-value',\n    },\n    body: {\n      temperature: 0.7,\n      max_tokens: 100,\n      user_id: '123',\n    },\n    metadata: {\n      userId: 'user123',\n      sessionId: 'session456',\n    },\n  },\n);\n\nThe request-level options are merged with hook-level options, with request-level options taking precedence. On your server side, you can handle the request with this additional information.\n\nSetting custom body fields per request\n\nYou can configure custom body fields on a per-request basis using the second parameter of the sendMessage function. This is useful if you want to pass in additional information to your backend that is not part of the message list.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage } = useChat();\n  const [input, setInput] = useState('');\n\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}:{' '}\n          {m.parts.map((part, index) =>\n            part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n          )}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={event => {\n          event.preventDefault();\n          if (input.trim()) {\n            sendMessage(\n              { text: input },\n              {\n                body: {\n                  customKey: 'customValue',\n                },\n              },\n            );\n            setInput('');\n          }\n        }}\n      >\n        <input value={input} onChange={e => setInput(e.target.value)} />\n      </form>\n    </div>\n  );\n}\n\nYou can retrieve these custom fields on your server side by destructuring the request body:\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  // Extract additional information (\"customKey\") from the body of the request:\n  const { messages, customKey }: { messages: UIMessage[]; customKey: string } =\n    await req.json();\n  //...\n}\nMessage Metadata\n\nYou can attach custom metadata to messages for tracking information like timestamps, model details, and token usage.\n\n// Server: Send metadata about the message\nreturn result.toUIMessageStreamResponse({\n  messageMetadata: ({ part }) => {\n    if (part.type === 'start') {\n      return {\n        createdAt: Date.now(),\n        model: 'gpt-4o',\n      };\n    }\n\n\n    if (part.type === 'finish') {\n      return {\n        totalTokens: part.totalUsage.totalTokens,\n      };\n    }\n  },\n});\n// Client: Access metadata via message.metadata\n{\n  messages.map(message => (\n    <div key={message.id}>\n      {message.role}:{' '}\n      {message.metadata?.createdAt &&\n        new Date(message.metadata.createdAt).toLocaleTimeString()}\n      {/* Render message content */}\n      {message.parts.map((part, index) =>\n        part.type === 'text' ? <span key={index}>{part.text}</span> : null,\n      )}\n      {/* Show token count if available */}\n      {message.metadata?.totalTokens && (\n        <span>{message.metadata.totalTokens} tokens</span>\n      )}\n    </div>\n  ));\n}\n\nFor complete examples with type safety and advanced use cases, see the Message Metadata documentation.\n\nTransport Configuration\n\nYou can configure custom transport behavior using the transport option to customize how messages are sent to your API:\n\napp/page.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  const { messages, sendMessage } = useChat({\n    id: 'my-chat',\n    transport: new DefaultChatTransport({\n      prepareSendMessagesRequest: ({ id, messages }) => {\n        return {\n          body: {\n            id,\n            message: messages[messages.length - 1],\n          },\n        };\n      },\n    }),\n  });\n\n\n  // ... rest of your component\n}\n\nThe corresponding API route receives the custom request format:\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  const { id, message } = await req.json();\n\n\n  // Load existing messages and add the new one\n  const messages = await loadMessages(id);\n  messages.push(message);\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nAdvanced: Trigger-based routing\n\nFor more complex scenarios like message regeneration, you can use trigger-based routing:\n\napp/page.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, regenerate } = useChat({\n    id: 'my-chat',\n    transport: new DefaultChatTransport({\n      prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\n        if (trigger === 'submit-user-message') {\n          return {\n            body: {\n              trigger: 'submit-user-message',\n              id,\n              message: messages[messages.length - 1],\n              messageId,\n            },\n          };\n        } else if (trigger === 'regenerate-assistant-message') {\n          return {\n            body: {\n              trigger: 'regenerate-assistant-message',\n              id,\n              messageId,\n            },\n          };\n        }\n        throw new Error(`Unsupported trigger: ${trigger}`);\n      },\n    }),\n  });\n\n\n  // ... rest of your component\n}\n\nThe corresponding API route would handle different triggers:\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  const { trigger, id, message, messageId } = await req.json();\n\n\n  const chat = await readChat(id);\n  let messages = chat.messages;\n\n\n  if (trigger === 'submit-user-message') {\n    // Handle new user message\n    messages = [...messages, message];\n  } else if (trigger === 'regenerate-assistant-message') {\n    // Handle message regeneration - remove messages after messageId\n    const messageIndex = messages.findIndex(m => m.id === messageId);\n    if (messageIndex !== -1) {\n      messages = messages.slice(0, messageIndex);\n    }\n  }\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nTo learn more about building custom transports, refer to the Transport API documentation.\n\nControlling the response stream\n\nWith streamText, you can control how error messages and usage information are sent back to the client.\n\nError Messages\n\nBy default, the error message is masked for security reasons. The default error message is \"An error occurred.\" You can forward error messages or send your own error message by providing a getErrorMessage function:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4.1'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    onError: error => {\n      if (error == null) {\n        return 'unknown error';\n      }\n\n\n      if (typeof error === 'string') {\n        return error;\n      }\n\n\n      if (error instanceof Error) {\n        return error.message;\n      }\n\n\n      return JSON.stringify(error);\n    },\n  });\n}\nUsage Information\n\nTrack token consumption and resource usage with message metadata:\n\nDefine a custom metadata type with usage fields (optional, for type safety)\nAttach usage data using messageMetadata in your response\nDisplay usage metrics in your UI components\n\nUsage data is attached as metadata to messages and becomes available once the model completes its response generation.\n\nimport { openai } from '@ai-sdk/openai';\nimport {\n  convertToModelMessages,\n  streamText,\n  UIMessage,\n  type LanguageModelUsage,\n} from 'ai';\n\n\n// Create a new metadata type (optional for type-safety)\ntype MyMetadata = {\n  totalUsage: LanguageModelUsage;\n};\n\n\n// Create a new custom message type with your own metadata\nexport type MyUIMessage = UIMessage<MyMetadata>;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: MyUIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    messageMetadata: ({ part }) => {\n      // Send total usage when generation is finished\n      if (part.type === 'finish') {\n        return { totalUsage: part.totalUsage };\n      }\n    },\n  });\n}\n\nThen, on the client, you can access the message-level metadata.\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport type { MyUIMessage } from './api/chat/route';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  // Use custom message type defined on the server (optional for type-safety)\n  const { messages } = useChat<MyUIMessage>({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(m => (\n        <div key={m.id} className=\"whitespace-pre-wrap\">\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.parts.map(part => {\n            if (part.type === 'text') {\n              return part.text;\n            }\n          })}\n          {/* Render usage via metadata */}\n          {m.metadata?.totalUsage && (\n            <div>Total usage: {m.metadata?.totalUsage.totalTokens} tokens</div>\n          )}\n        </div>\n      ))}\n    </div>\n  );\n}\n\nYou can also access your metadata from the onFinish callback of useChat:\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport type { MyUIMessage } from './api/chat/route';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  // Use custom message type defined on the server (optional for type-safety)\n  const { messages } = useChat<MyUIMessage>({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n    onFinish: ({ message }) => {\n      // Access message metadata via onFinish callback\n      console.log(message.metadata?.totalUsage);\n    },\n  });\n}\nText Streams\n\nuseChat can handle plain text streams by setting the streamProtocol option to text:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { TextStreamChatTransport } from 'ai';\n\n\nexport default function Chat() {\n  const { messages } = useChat({\n    transport: new TextStreamChatTransport({\n      api: '/api/chat',\n    }),\n  });\n\n\n  return <>...</>;\n}\n\nThis configuration also works with other backend servers that stream plain text. Check out the stream protocol guide for more information.\n\nWhen using TextStreamChatTransport, tool calls, usage information and finish reasons are not available.\n\nReasoning\n\nSome models such as as DeepSeek deepseek-reasoner and Anthropic claude-3-7-sonnet-20250219 support reasoning tokens. These tokens are typically sent before the message content. You can forward them to the client with the sendReasoning option:\n\napp/api/chat/route.ts\nimport { deepseek } from '@ai-sdk/deepseek';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: deepseek('deepseek-reasoner'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    sendReasoning: true,\n  });\n}\n\nOn the client side, you can access the reasoning parts of the message object.\n\nReasoning parts have a text property that contains the reasoning content.\n\napp/page.tsx\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n    {message.parts.map((part, index) => {\n      // text parts:\n      if (part.type === 'text') {\n        return <div key={index}>{part.text}</div>;\n      }\n\n\n      // reasoning parts:\n      if (part.type === 'reasoning') {\n        return <pre key={index}>{part.text}</pre>;\n      }\n    })}\n  </div>\n));\nSources\n\nSome providers such as Perplexity and Google Generative AI include sources in the response.\n\nCurrently sources are limited to web pages that ground the response. You can forward them to the client with the sendSources option:\n\napp/api/chat/route.ts\nimport { perplexity } from '@ai-sdk/perplexity';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: perplexity('sonar-pro'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    sendSources: true,\n  });\n}\n\nOn the client side, you can access source parts of the message object. There are two types of sources: source-url for web pages and source-document for documents. Here is an example that renders both types of sources:\n\napp/page.tsx\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n\n\n    {/* Render URL sources */}\n    {message.parts\n      .filter(part => part.type === 'source-url')\n      .map(part => (\n        <span key={`source-${part.id}`}>\n          [\n          <a href={part.url} target=\"_blank\">\n            {part.title ?? new URL(part.url).hostname}\n          </a>\n          ]\n        </span>\n      ))}\n\n\n    {/* Render document sources */}\n    {message.parts\n      .filter(part => part.type === 'source-document')\n      .map(part => (\n        <span key={`source-${part.id}`}>\n          [<span>{part.title ?? `Document ${part.id}`}</span>]\n        </span>\n      ))}\n  </div>\n));\nImage Generation\n\nSome models such as Google gemini-2.5-flash-image-preview support image generation. When images are generated, they are exposed as files to the client. On the client side, you can access file parts of the message object and render them as images.\n\napp/page.tsx\nmessages.map(message => (\n  <div key={message.id}>\n    {message.role === 'user' ? 'User: ' : 'AI: '}\n    {message.parts.map((part, index) => {\n      if (part.type === 'text') {\n        return <div key={index}>{part.text}</div>;\n      } else if (part.type === 'file' && part.mediaType.startsWith('image/')) {\n        return <img key={index} src={part.url} alt=\"Generated image\" />;\n      }\n    })}\n  </div>\n));\nAttachments\n\nThe useChat hook supports sending file attachments along with a message as well as rendering them on the client. This can be useful for building applications that involve sending images, files, or other media content to the AI provider.\n\nThere are two ways to send files with a message: using a FileList object from file inputs or using an array of file objects.\n\nFileList\n\nBy using FileList, you can send multiple files as attachments along with a message using the file input element. The useChat hook will automatically convert them into data URLs and send them to the AI provider.\n\nCurrently, only image/* and text/* content types get automatically converted into multi-modal content parts. You will need to handle other content types manually.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useRef, useState } from 'react';\n\n\nexport default function Page() {\n  const { messages, sendMessage, status } = useChat();\n\n\n  const [input, setInput] = useState('');\n  const [files, setFiles] = useState<FileList | undefined>(undefined);\n  const fileInputRef = useRef<HTMLInputElement>(null);\n\n\n  return (\n    <div>\n      <div>\n        {messages.map(message => (\n          <div key={message.id}>\n            <div>{`${message.role}: `}</div>\n\n\n            <div>\n              {message.parts.map((part, index) => {\n                if (part.type === 'text') {\n                  return <span key={index}>{part.text}</span>;\n                }\n\n\n                if (\n                  part.type === 'file' &&\n                  part.mediaType?.startsWith('image/')\n                ) {\n                  return <img key={index} src={part.url} alt={part.filename} />;\n                }\n\n\n                return null;\n              })}\n            </div>\n          </div>\n        ))}\n      </div>\n\n\n      <form\n        onSubmit={event => {\n          event.preventDefault();\n          if (input.trim()) {\n            sendMessage({\n              text: input,\n              files,\n            });\n            setInput('');\n            setFiles(undefined);\n\n\n            if (fileInputRef.current) {\n              fileInputRef.current.value = '';\n            }\n          }\n        }}\n      >\n        <input\n          type=\"file\"\n          onChange={event => {\n            if (event.target.files) {\n              setFiles(event.target.files);\n            }\n          }}\n          multiple\n          ref={fileInputRef}\n        />\n        <input\n          value={input}\n          placeholder=\"Send message...\"\n          onChange={e => setInput(e.target.value)}\n          disabled={status !== 'ready'}\n        />\n      </form>\n    </div>\n  );\n}\nFile Objects\n\nYou can also send files as objects along with a message. This can be useful for sending pre-uploaded files or data URLs.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\nimport { FileUIPart } from 'ai';\n\n\nexport default function Page() {\n  const { messages, sendMessage, status } = useChat();\n\n\n  const [input, setInput] = useState('');\n  const [files] = useState<FileUIPart[]>([\n    {\n      type: 'file',\n      filename: 'earth.png',\n      mediaType: 'image/png',\n      url: 'https://example.com/earth.png',\n    },\n    {\n      type: 'file',\n      filename: 'moon.png',\n      mediaType: 'image/png',\n      url: 'data:image/png;base64,iVBORw0KGgo...',\n    },\n  ]);\n\n\n  return (\n    <div>\n      <div>\n        {messages.map(message => (\n          <div key={message.id}>\n            <div>{`${message.role}: `}</div>\n\n\n            <div>\n              {message.parts.map((part, index) => {\n                if (part.type === 'text') {\n                  return <span key={index}>{part.text}</span>;\n                }\n\n\n                if (\n                  part.type === 'file' &&\n                  part.mediaType?.startsWith('image/')\n                ) {\n                  return <img key={index} src={part.url} alt={part.filename} />;\n                }\n\n\n                return null;\n              })}\n            </div>\n          </div>\n        ))}\n      </div>\n\n\n      <form\n        onSubmit={event => {\n          event.preventDefault();\n          if (input.trim()) {\n            sendMessage({\n              text: input,\n              files,\n            });\n            setInput('');\n          }\n        }}\n      >\n        <input\n          value={input}\n          placeholder=\"Send message...\"\n          onChange={e => setInput(e.target.value)}\n          disabled={status !== 'ready'}\n        />\n      </form>\n    </div>\n  );\n}\nType Inference for Tools\n\nWhen working with tools in TypeScript, AI SDK UI provides type inference helpers to ensure type safety for your tool inputs and outputs.\n\nInferUITool\n\nThe InferUITool type helper infers the input and output types of a single tool for use in UI messages:\n\nimport { InferUITool } from 'ai';\nimport { z } from 'zod';\n\n\nconst weatherTool = {\n  description: 'Get the current weather',\n  inputSchema: z.object({\n    location: z.string().describe('The city and state'),\n  }),\n  execute: async ({ location }) => {\n    return `The weather in ${location} is sunny.`;\n  },\n};\n\n\n// Infer the types from the tool\ntype WeatherUITool = InferUITool<typeof weatherTool>;\n// This creates a type with:\n// {\n//   input: { location: string };\n//   output: string;\n// }\nInferUITools\n\nThe InferUITools type helper infers the input and output types of a ToolSet:\n\nimport { InferUITools, ToolSet } from 'ai';\nimport { z } from 'zod';\n\n\nconst tools = {\n  weather: {\n    description: 'Get the current weather',\n    inputSchema: z.object({\n      location: z.string().describe('The city and state'),\n    }),\n    execute: async ({ location }) => {\n      return `The weather in ${location} is sunny.`;\n    },\n  },\n  calculator: {\n    description: 'Perform basic arithmetic',\n    inputSchema: z.object({\n      operation: z.enum(['add', 'subtract', 'multiply', 'divide']),\n      a: z.number(),\n      b: z.number(),\n    }),\n    execute: async ({ operation, a, b }) => {\n      switch (operation) {\n        case 'add':\n          return a + b;\n        case 'subtract':\n          return a - b;\n        case 'multiply':\n          return a * b;\n        case 'divide':\n          return a / b;\n      }\n    },\n  },\n} satisfies ToolSet;\n\n\n// Infer the types from the tool set\ntype MyUITools = InferUITools<typeof tools>;\n// This creates a type with:\n// {\n//   weather: { input: { location: string }; output: string };\n//   calculator: { input: { operation: 'add' | 'subtract' | 'multiply' | 'divide'; a: number; b: number }; output: number };\n// }\nUsing Inferred Types\n\nYou can use these inferred types to create a custom UIMessage type and pass it to various AI SDK UI functions:\n\nimport { InferUITools, UIMessage, UIDataTypes } from 'ai';\n\n\ntype MyUITools = InferUITools<typeof tools>;\ntype MyUIMessage = UIMessage<never, UIDataTypes, MyUITools>;\n\nPass the custom type to useChat or createUIMessageStream:\n\nimport { useChat } from '@ai-sdk/react';\nimport { createUIMessageStream } from 'ai';\nimport type { MyUIMessage } from './types';\n\n\n// With useChat\nconst { messages } = useChat<MyUIMessage>();\n\n\n// With createUIMessageStream\nconst stream = createUIMessageStream<MyUIMessage>(/* ... */);\n\nThis provides full type safety for tool inputs and outputs on the client and server.\n\nPrevious\nOverview\nNext\nChatbot Message Persistence"
  },
  {
    "title": "AI SDK UI: Chatbot Message Persistence",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-message-persistence",
    "html": "AI SDK UI\nChatbot Message Persistence\nCopy markdown\nChatbot Message Persistence\n\nBeing able to store and load chat messages is crucial for most AI chatbots. In this guide, we'll show how to implement message persistence with useChat and streamText.\n\nThis guide does not cover authorization, error handling, or other real-world considerations. It is intended to be a simple example of how to implement message persistence.\n\nStarting a new chat\n\nWhen the user navigates to the chat page without providing a chat ID, we need to create a new chat and redirect to the chat page with the new chat ID.\n\napp/chat/page.tsx\nimport { redirect } from 'next/navigation';\nimport { createChat } from '@util/chat-store';\n\n\nexport default async function Page() {\n  const id = await createChat(); // create a new chat\n  redirect(`/chat/${id}`); // redirect to chat page, see below\n}\n\nOur example chat store implementation uses files to store the chat messages. In a real-world application, you would use a database or a cloud storage service, and get the chat ID from the database. That being said, the function interfaces are designed to be easily replaced with other implementations.\n\nutil/chat-store.ts\nimport { generateId } from 'ai';\nimport { existsSync, mkdirSync } from 'fs';\nimport { writeFile } from 'fs/promises';\nimport path from 'path';\n\n\nexport async function createChat(): Promise<string> {\n  const id = generateId(); // generate a unique chat ID\n  await writeFile(getChatFile(id), '[]'); // create an empty chat file\n  return id;\n}\n\n\nfunction getChatFile(id: string): string {\n  const chatDir = path.join(process.cwd(), '.chats');\n  if (!existsSync(chatDir)) mkdirSync(chatDir, { recursive: true });\n  return path.join(chatDir, `${id}.json`);\n}\nLoading an existing chat\n\nWhen the user navigates to the chat page with a chat ID, we need to load the chat messages from storage.\n\nThe loadChat function in our file-based chat store is implemented as follows:\n\nutil/chat-store.ts\nimport { UIMessage } from 'ai';\nimport { readFile } from 'fs/promises';\n\n\nexport async function loadChat(id: string): Promise<UIMessage[]> {\n  return JSON.parse(await readFile(getChatFile(id), 'utf8'));\n}\n\n\n// ... rest of the file\nValidating messages on the server\n\nWhen processing messages on the server that contain tool calls, custom metadata, or data parts, you should validate them using validateUIMessages before sending them to the model.\n\nValidation with tools\n\nWhen your messages include tool calls, validate them against your tool definitions:\n\napp/api/chat/route.ts\nimport {\n  convertToModelMessages,\n  streamText,\n  UIMessage,\n  validateUIMessages,\n  tool,\n} from 'ai';\nimport { z } from 'zod';\nimport { loadChat, saveChat } from '@util/chat-store';\nimport { openai } from '@ai-sdk/openai';\nimport { dataPartsSchema, metadataSchema } from '@util/schemas';\n\n\n// Define your tools\nconst tools = {\n  weather: tool({\n    description: 'Get weather information',\n    parameters: z.object({\n      location: z.string(),\n      units: z.enum(['celsius', 'fahrenheit']),\n    }),\n    execute: async ({ location, units }) => {\n      /* tool implementation */\n    },\n  }),\n  // other tools\n};\n\n\nexport async function POST(req: Request) {\n  const { message, id } = await req.json();\n\n\n  // Load previous messages from database\n  const previousMessages = await loadChat(id);\n\n\n  // Append new message to previousMessages messages\n  const messages = [...previousMessages, message];\n\n\n  // Validate loaded messages against\n  // tools, data parts schema, and metadata schema\n  const validatedMessages = await validateUIMessages({\n    messages,\n    tools, // Ensures tool calls in messages match current schemas\n    dataPartsSchema,\n    metadataSchema,\n  });\n\n\n  const result = streamText({\n    model: openai('gpt-4o-mini'),\n    messages: convertToModelMessages(validatedMessages),\n    tools,\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    onFinish: ({ messages }) => {\n      saveChat({ chatId: id, messages });\n    },\n  });\n}\nHandling validation errors\n\nHandle validation errors gracefully when messages from the database don't match current schemas:\n\napp/api/chat/route.ts\nimport {\n  convertToModelMessages,\n  streamText,\n  validateUIMessages,\n  TypeValidationError,\n} from 'ai';\nimport { type MyUIMessage } from '@/types';\n\n\nexport async function POST(req: Request) {\n  const { message, id } = await req.json();\n\n\n  // Load and validate messages from database\n  let validatedMessages: MyUIMessage[];\n\n\n  try {\n    const previousMessages = await loadMessagesFromDB(id);\n    validatedMessages = await validateUIMessages({\n      // append the new message to the previous messages:\n      messages: [...previousMessages, message],\n      tools,\n      metadataSchema,\n    });\n  } catch (error) {\n    if (error instanceof TypeValidationError) {\n      // Log validation error for monitoring\n      console.error('Database messages validation failed:', error);\n      // Could implement message migration or filtering here\n      // For now, start with empty history\n      validatedMessages = [];\n    } else {\n      throw error;\n    }\n  }\n\n\n  // Continue with validated messages...\n}\nDisplaying the chat\n\nOnce messages are loaded from storage, you can display them in your chat UI. Here's how to set up the page component and the chat display:\n\napp/chat/[id]/page.tsx\nimport { loadChat } from '@util/chat-store';\nimport Chat from '@ui/chat';\n\n\nexport default async function Page(props: { params: Promise<{ id: string }> }) {\n  const { id } = await props.params;\n  const messages = await loadChat(id);\n  return <Chat id={id} initialMessages={messages} />;\n}\n\nThe chat component uses the useChat hook to manage the conversation:\n\nui/chat.tsx\n'use client';\n\n\nimport { UIMessage, useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat({\n  id,\n  initialMessages,\n}: { id?: string | undefined; initialMessages?: UIMessage[] } = {}) {\n  const [input, setInput] = useState('');\n  const { sendMessage, messages } = useChat({\n    id, // use the provided chat ID\n    messages: initialMessages, // load initial messages\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    if (input.trim()) {\n      sendMessage({ text: input });\n      setInput('');\n    }\n  };\n\n\n  // simplified rendering code, extend as needed:\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role === 'user' ? 'User: ' : 'AI: '}\n          {m.parts\n            .map(part => (part.type === 'text' ? part.text : ''))\n            .join('')}\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\nStoring messages\n\nuseChat sends the chat id and the messages to the backend.\n\nThe useChat message format is different from the ModelMessage format. The useChat message format is designed for frontend display, and contains additional fields such as id and createdAt. We recommend storing the messages in the useChat message format.\n\nWhen loading messages from storage that contain tools, metadata, or custom data parts, validate them using validateUIMessages before processing (see the validation section above).\n\nStoring messages is done in the onFinish callback of the toUIMessageStreamResponse function. onFinish receives the complete messages including the new AI response as UIMessage[].\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { saveChat } from '@util/chat-store';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\n    await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o-mini'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    onFinish: ({ messages }) => {\n      saveChat({ chatId, messages });\n    },\n  });\n}\n\nThe actual storage of the messages is done in the saveChat function, which in our file-based chat store is implemented as follows:\n\nutil/chat-store.ts\nimport { UIMessage } from 'ai';\nimport { writeFile } from 'fs/promises';\n\n\nexport async function saveChat({\n  chatId,\n  messages,\n}: {\n  chatId: string;\n  messages: UIMessage[];\n}): Promise<void> {\n  const content = JSON.stringify(messages, null, 2);\n  await writeFile(getChatFile(chatId), content);\n}\n\n\n// ... rest of the file\nMessage IDs\n\nIn addition to a chat ID, each message has an ID. You can use this message ID to e.g. manipulate individual messages.\n\nClient-side vs Server-side ID Generation\n\nBy default, message IDs are generated client-side:\n\nUser message IDs are generated by the useChat hook on the client\nAI response message IDs are generated by streamText on the server\n\nFor applications without persistence, client-side ID generation works perfectly. However, for persistence, you need server-side generated IDs to ensure consistency across sessions and prevent ID conflicts when messages are stored and retrieved.\n\nSetting Up Server-side ID Generation\n\nWhen implementing persistence, you have two options for generating server-side IDs:\n\nUsing generateMessageId in toUIMessageStreamResponse\nSetting IDs in your start message part with createUIMessageStream\nOption 1: Using generateMessageId in toUIMessageStreamResponse\n\nYou can control the ID format by providing ID generators using createIdGenerator():\n\napp/api/chat/route.ts\nimport { createIdGenerator, streamText } from 'ai';\n\n\nexport async function POST(req: Request) {\n  // ...\n  const result = streamText({\n    // ...\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    // Generate consistent server-side IDs for persistence:\n    generateMessageId: createIdGenerator({\n      prefix: 'msg',\n      size: 16,\n    }),\n    onFinish: ({ messages }) => {\n      saveChat({ chatId, messages });\n    },\n  });\n}\nOption 2: Setting IDs with createUIMessageStream\n\nAlternatively, you can use createUIMessageStream to control the message ID by writing a start message part:\n\napp/api/chat/route.ts\nimport {\n  generateId,\n  streamText,\n  createUIMessageStream,\n  createUIMessageStreamResponse,\n} from 'ai';\n\n\nexport async function POST(req: Request) {\n  const { messages, chatId } = await req.json();\n\n\n  const stream = createUIMessageStream({\n    execute: ({ writer }) => {\n      // Write start message part with custom ID\n      writer.write({\n        type: 'start',\n        messageId: generateId(), // Generate server-side ID for persistence\n      });\n\n\n      const result = streamText({\n        model: openai('gpt-4o-mini'),\n        messages: convertToModelMessages(messages),\n      });\n\n\n      writer.merge(result.toUIMessageStream({ sendStart: false })); // omit start message part\n    },\n    originalMessages: messages,\n    onFinish: ({ responseMessage }) => {\n      // save your chat here\n    },\n  });\n\n\n  return createUIMessageStreamResponse({ stream });\n}\n\nFor client-side applications that don't require persistence, you can still customize client-side ID generation:\n\nui/chat.tsx\nimport { createIdGenerator } from 'ai';\nimport { useChat } from '@ai-sdk/react';\n\n\nconst { ... } = useChat({\n  generateId: createIdGenerator({\n    prefix: 'msgc',\n    size: 16,\n  }),\n  // ...\n});\nSending only the last message\n\nOnce you have implemented message persistence, you might want to send only the last message to the server. This reduces the amount of data sent to the server on each request and can improve performance.\n\nTo achieve this, you can provide a prepareSendMessagesRequest function to the transport. This function receives the messages and the chat ID, and returns the request body to be sent to the server.\n\nui/chat.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst {\n  // ...\n} = useChat({\n  // ...\n  transport: new DefaultChatTransport({\n    api: '/api/chat',\n    // only send the last message to the server:\n    prepareSendMessagesRequest({ messages, id }) {\n      return { body: { message: messages[messages.length - 1], id } };\n    },\n  }),\n});\n\nOn the server, you can then load the previous messages and append the new message to the previous messages. If your messages contain tools, metadata, or custom data parts, you should validate them:\n\napp/api/chat/route.ts\nimport { convertToModelMessages, UIMessage, validateUIMessages } from 'ai';\n// import your tools and schemas\n\n\nexport async function POST(req: Request) {\n  // get the last message from the client:\n  const { message, id } = await req.json();\n\n\n  // load the previous messages from the server:\n  const previousMessages = await loadChat(id);\n\n\n  // validate messages if they contain tools, metadata, or data parts:\n  const validatedMessages = await validateUIMessages({\n    // append the new message to the previous messages:\n    messages: [...previousMessages, message],\n    tools, // if using tools\n    metadataSchema, // if using custom metadata\n    dataSchemas, // if using custom data parts\n  });\n\n\n  const result = streamText({\n    // ...\n    messages: convertToModelMessages(validatedMessages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: validatedMessages,\n    onFinish: ({ messages }) => {\n      saveChat({ chatId: id, messages });\n    },\n  });\n}\nHandling client disconnects\n\nBy default, the AI SDK streamText function uses backpressure to the language model provider to prevent the consumption of tokens that are not yet requested.\n\nHowever, this means that when the client disconnects, e.g. by closing the browser tab or because of a network issue, the stream from the LLM will be aborted and the conversation may end up in a broken state.\n\nAssuming that you have a storage solution in place, you can use the consumeStream method to consume the stream on the backend, and then save the result as usual. consumeStream effectively removes the backpressure, meaning that the result is stored even when the client has already disconnected.\n\napp/api/chat/route.ts\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\nimport { saveChat } from '@util/chat-store';\n\n\nexport async function POST(req: Request) {\n  const { messages, chatId }: { messages: UIMessage[]; chatId: string } =\n    await req.json();\n\n\n  const result = streamText({\n    model,\n    messages: convertToModelMessages(messages),\n  });\n\n\n  // consume the stream to ensure it runs to completion & triggers onFinish\n  // even when the client response is aborted:\n  result.consumeStream(); // no await\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    onFinish: ({ messages }) => {\n      saveChat({ chatId, messages });\n    },\n  });\n}\n\nWhen the client reloads the page after a disconnect, the chat will be restored from the storage solution.\n\nIn production applications, you would also track the state of the request (in progress, complete) in your stored messages and use it on the client to cover the case where the client reloads the page after a disconnection, but the streaming is not yet complete.\n\nFor more robust handling of disconnects, you may want to add resumability on disconnects. Check out the Chatbot Resume Streams documentation to learn more.\n\nPrevious\nChatbot\nNext\nChatbot Resume Streams"
  },
  {
    "title": "AI SDK UI: Chatbot Resume Streams",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-resume-streams",
    "html": "AI SDK UI\nChatbot Resume Streams\nCopy markdown\nChatbot Resume Streams\n\nuseChat supports resuming ongoing streams after page reloads. Use this feature to build applications with long-running generations.\n\nStream resumption is not compatible with abort functionality. Closing a tab or refreshing the page triggers an abort signal that will break the resumption mechanism. Do not use resume: true if you need abort functionality in your application. See troubleshooting for more details.\n\nHow stream resumption works\n\nStream resumption requires persistence for messages and active streams in your application. The AI SDK provides tools to connect to storage, but you need to set up the storage yourself.\n\nThe AI SDK provides:\n\nA resume option in useChat that automatically reconnects to active streams\nAccess to the outgoing stream through the consumeSseStream callback\nAutomatic HTTP requests to your resume endpoints\n\nYou build:\n\nStorage to track which stream belongs to each chat\nRedis to store the UIMessage stream\nTwo API endpoints: POST to create streams, GET to resume them\nIntegration with resumable-stream\n to manage Redis storage\nPrerequisites\n\nTo implement resumable streams in your chat application, you need:\n\nThe resumable-stream package - Handles the publisher/subscriber mechanism for streams\nA Redis instance - Stores stream data (e.g. Redis through Vercel\n)\nA persistence layer - Tracks which stream ID is active for each chat (e.g. database)\nImplementation\n1. Client-side: Enable stream resumption\n\nUse the resume option in the useChat hook to enable stream resumption. When resume is true, the hook automatically attempts to reconnect to any active stream for the chat on mount:\n\napp/chat/[chatId]/chat.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport, type UIMessage } from 'ai';\n\n\nexport function Chat({\n  chatData,\n  resume = false,\n}: {\n  chatData: { id: string; messages: UIMessage[] };\n  resume?: boolean;\n}) {\n  const { messages, sendMessage, status } = useChat({\n    id: chatData.id,\n    messages: chatData.messages,\n    resume, // Enable automatic stream resumption\n    transport: new DefaultChatTransport({\n      // You must send the id of the chat\n      prepareSendMessagesRequest: ({ id, messages }) => {\n        return {\n          body: {\n            id,\n            message: messages[messages.length - 1],\n          },\n        };\n      },\n    }),\n  });\n\n\n  return <div>{/* Your chat UI */}</div>;\n}\n\nYou must send the chat ID with each request (see prepareSendMessagesRequest).\n\nWhen you enable resume, the useChat hook makes a GET request to /api/chat/[id]/stream on mount to check for and resume any active streams.\n\nLet's start by creating the POST handler to create the resumable stream.\n\n2. Create the POST handler\n\nThe POST handler creates resumable streams using the consumeSseStream callback:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { readChat, saveChat } from '@util/chat-store';\nimport {\n  convertToModelMessages,\n  generateId,\n  streamText,\n  type UIMessage,\n} from 'ai';\nimport { after } from 'next/server';\nimport { createResumableStreamContext } from 'resumable-stream';\n\n\nexport async function POST(req: Request) {\n  const {\n    message,\n    id,\n  }: {\n    message: UIMessage | undefined;\n    id: string;\n  } = await req.json();\n\n\n  const chat = await readChat(id);\n  let messages = chat.messages;\n\n\n  messages = [...messages, message!];\n\n\n  // Clear any previous active stream and save the user message\n  saveChat({ id, messages, activeStreamId: null });\n\n\n  const result = streamText({\n    model: openai('gpt-4o-mini'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages,\n    generateMessageId: generateId,\n    onFinish: ({ messages }) => {\n      // Clear the active stream when finished\n      saveChat({ id, messages, activeStreamId: null });\n    },\n    async consumeSseStream({ stream }) {\n      const streamId = generateId();\n\n\n      // Create a resumable stream from the SSE stream\n      const streamContext = createResumableStreamContext({ waitUntil: after });\n      await streamContext.createNewResumableStream(streamId, () => stream);\n\n\n      // Update the chat with the active stream ID\n      saveChat({ id, activeStreamId: streamId });\n    },\n  });\n}\n3. Implement the GET handler\n\nCreate a GET handler at /api/chat/[id]/stream that:\n\nReads the chat ID from the route params\nLoads the chat data to check for an active stream\nReturns 204 (No Content) if no stream is active\nResumes the existing stream if one is found\napp/api/chat/[id]/stream/route.ts\nimport { readChat } from '@util/chat-store';\nimport { UI_MESSAGE_STREAM_HEADERS } from 'ai';\nimport { after } from 'next/server';\nimport { createResumableStreamContext } from 'resumable-stream';\n\n\nexport async function GET(\n  _: Request,\n  { params }: { params: Promise<{ id: string }> },\n) {\n  const { id } = await params;\n\n\n  const chat = await readChat(id);\n\n\n  if (chat.activeStreamId == null) {\n    // no content response when there is no active stream\n    return new Response(null, { status: 204 });\n  }\n\n\n  const streamContext = createResumableStreamContext({\n    waitUntil: after,\n  });\n\n\n  return new Response(\n    await streamContext.resumeExistingStream(chat.activeStreamId),\n    { headers: UI_MESSAGE_STREAM_HEADERS },\n  );\n}\n\nThe after function from Next.js allows work to continue after the response has been sent. This ensures that the resumable stream persists in Redis even after the initial response is returned to the client, enabling reconnection later.\n\nHow it works\nRequest lifecycle\n\nThe diagram above shows the complete lifecycle of a resumable stream:\n\nStream creation: When you send a new message, the POST handler uses streamText to generate the response. The consumeSseStream callback creates a resumable stream with a unique ID and stores it in Redis through the resumable-stream package\nStream tracking: Your persistence layer saves the activeStreamId in the chat data\nClient reconnection: When the client reconnects (page reload), the resume option triggers a GET request to /api/chat/[id]/stream\nStream recovery: The GET handler checks for an activeStreamId and uses resumeExistingStream to reconnect. If no active stream exists, it returns a 204 (No Content) response\nCompletion cleanup: When the stream finishes, the onFinish callback clears the activeStreamId by setting it to null\nCustomize the resume endpoint\n\nBy default, the useChat hook makes a GET request to /api/chat/[id]/stream when resuming. Customize this endpoint, credentials, and headers, using the prepareReconnectToStreamRequest option in DefaultChatTransport:\n\napp/chat/[chatId]/chat.tsx\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nexport function Chat({ chatData, resume }) {\n  const { messages, sendMessage } = useChat({\n    id: chatData.id,\n    messages: chatData.messages,\n    resume,\n    transport: new DefaultChatTransport({\n      // Customize reconnect settings (optional)\n      prepareReconnectToStreamRequest: ({ id }) => {\n        return {\n          api: `/api/chat/${id}/stream`, // Default pattern\n          // Or use a different pattern:\n          // api: `/api/streams/${id}/resume`,\n          // api: `/api/resume-chat?id=${id}`,\n          credentials: 'include', // Include cookies/auth\n          headers: {\n            Authorization: 'Bearer token',\n            'X-Custom-Header': 'value',\n          },\n        };\n      },\n    }),\n  });\n\n\n  return <div>{/* Your chat UI */}</div>;\n}\n\nThis lets you:\n\nMatch your existing API route structure\nAdd query parameters or custom paths\nIntegrate with different backend architectures\nImportant considerations\nIncompatibility with abort: Stream resumption is not compatible with abort functionality. Closing a tab or refreshing the page triggers an abort signal that will break the resumption mechanism. Do not use resume: true if you need abort functionality in your application\nStream expiration: Streams in Redis expire after a set time (configurable in the resumable-stream package)\nMultiple clients: Multiple clients can connect to the same stream simultaneously\nError handling: When no active stream exists, the GET handler returns a 204 (No Content) status code\nSecurity: Ensure proper authentication and authorization for both creating and resuming streams\nRace conditions: Clear the activeStreamId when starting a new stream to prevent resuming outdated streams\n\n\nView Example on GitHub\nPrevious\nChatbot Message Persistence\nNext\nChatbot Tool Usage"
  },
  {
    "title": "AI SDK UI: Chatbot Tool Usage",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
    "html": "AI SDK UI\nChatbot Tool Usage\nCopy markdown\nChatbot Tool Usage\n\nWith useChat and streamText, you can use tools in your chatbot application. The AI SDK supports three types of tools in this context:\n\nAutomatically executed server-side tools\nAutomatically executed client-side tools\nTools that require user interaction, such as confirmation dialogs\n\nThe flow is as follows:\n\nThe user enters a message in the chat UI.\nThe message is sent to the API route.\nIn your server side route, the language model generates tool calls during the streamText call.\nAll tool calls are forwarded to the client.\nServer-side tools are executed using their execute method and their results are forwarded to the client.\nClient-side tools that should be automatically executed are handled with the onToolCall callback. You must call addToolResult to provide the tool result.\nClient-side tool that require user interactions can be displayed in the UI. The tool calls and results are available as tool invocation parts in the parts property of the last assistant message.\nWhen the user interaction is done, addToolResult can be used to add the tool result to the chat.\nThe chat can be configured to automatically submit when all tool results are available using sendAutomaticallyWhen. This triggers another iteration of this flow.\n\nThe tool calls and tool executions are integrated into the assistant message as typed tool parts. A tool part is at first a tool call, and then it becomes a tool result when the tool is executed. The tool result contains all information about the tool call as well as the result of the tool execution.\n\nTool result submission can be configured using the sendAutomaticallyWhen option. You can use the lastAssistantMessageIsCompleteWithToolCalls helper to automatically submit when all tool results are available. This simplifies the client-side code while still allowing full control when needed.\n\nExample\n\nIn this example, we'll use three tools:\n\ngetWeatherInformation: An automatically executed server-side tool that returns the weather in a given city.\naskForConfirmation: A user-interaction client-side tool that asks the user for confirmation.\ngetLocation: An automatically executed client-side tool that returns a random city.\nAPI route\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\nimport { z } from 'zod';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      // server-side tool with execute function:\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        inputSchema: z.object({ city: z.string() }),\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n      // client-side tool that starts user interaction:\n      askForConfirmation: {\n        description: 'Ask the user for confirmation.',\n        inputSchema: z.object({\n          message: z.string().describe('The message to ask for confirmation.'),\n        }),\n      },\n      // client-side tool that is automatically executed on the client:\n      getLocation: {\n        description:\n          'Get the user location. Always ask for confirmation before using this tool.',\n        inputSchema: z.object({}),\n      },\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nClient-side page\n\nThe client-side page uses the useChat hook to create a chatbot application with real-time message streaming. Tool calls are displayed in the chat UI as typed tool parts. Please make sure to render the messages using the parts property of the message.\n\nThere are three things worth mentioning:\n\nThe onToolCall callback is used to handle client-side tools that should be automatically executed. In this example, the getLocation tool is a client-side tool that returns a random city. You call addToolResult to provide the result (without await to avoid potential deadlocks).\n\nAlways check if (toolCall.dynamic) first in your onToolCall handler. Without this check, TypeScript will throw an error like: Type 'string' is not assignable to type '\"toolName1\" | \"toolName2\"' when you try to use toolCall.toolName in addToolResult.\n\nThe sendAutomaticallyWhen option with lastAssistantMessageIsCompleteWithToolCalls helper automatically submits when all tool results are available.\n\nThe parts array of assistant messages contains tool parts with typed names like tool-askForConfirmation. The client-side tool askForConfirmation is displayed in the UI. It asks the user for confirmation and displays the result once the user confirms or denies the execution. The result is added to the chat using addToolResult with the tool parameter for type safety.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport {\n  DefaultChatTransport,\n  lastAssistantMessageIsCompleteWithToolCalls,\n} from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, addToolResult } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n\n\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n\n    // run client-side tools that are automatically executed:\n    async onToolCall({ toolCall }) {\n      // Check if it's a dynamic tool first for proper type narrowing\n      if (toolCall.dynamic) {\n        return;\n      }\n\n\n      if (toolCall.toolName === 'getLocation') {\n        const cities = ['New York', 'Los Angeles', 'Chicago', 'San Francisco'];\n\n\n        // No await - avoids potential deadlocks\n        addToolResult({\n          tool: 'getLocation',\n          toolCallId: toolCall.toolCallId,\n          output: cities[Math.floor(Math.random() * cities.length)],\n        });\n      }\n    },\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          <strong>{`${message.role}: `}</strong>\n          {message.parts.map(part => {\n            switch (part.type) {\n              // render text parts as simple text:\n              case 'text':\n                return part.text;\n\n\n              // for tool parts, use the typed tool part names:\n              case 'tool-askForConfirmation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  case 'input-streaming':\n                    return (\n                      <div key={callId}>Loading confirmation request...</div>\n                    );\n                  case 'input-available':\n                    return (\n                      <div key={callId}>\n                        {part.input.message}\n                        <div>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                tool: 'askForConfirmation',\n                                toolCallId: callId,\n                                output: 'Yes, confirmed.',\n                              })\n                            }\n                          >\n                            Yes\n                          </button>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                tool: 'askForConfirmation',\n                                toolCallId: callId,\n                                output: 'No, denied',\n                              })\n                            }\n                          >\n                            No\n                          </button>\n                        </div>\n                      </div>\n                    );\n                  case 'output-available':\n                    return (\n                      <div key={callId}>\n                        Location access allowed: {part.output}\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={callId}>Error: {part.errorText}</div>;\n                }\n                break;\n              }\n\n\n              case 'tool-getLocation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  case 'input-streaming':\n                    return (\n                      <div key={callId}>Preparing location request...</div>\n                    );\n                  case 'input-available':\n                    return <div key={callId}>Getting location...</div>;\n                  case 'output-available':\n                    return <div key={callId}>Location: {part.output}</div>;\n                  case 'output-error':\n                    return (\n                      <div key={callId}>\n                        Error getting location: {part.errorText}\n                      </div>\n                    );\n                }\n                break;\n              }\n\n\n              case 'tool-getWeatherInformation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  // example of pre-rendering streaming tool inputs:\n                  case 'input-streaming':\n                    return (\n                      <pre key={callId}>{JSON.stringify(part, null, 2)}</pre>\n                    );\n                  case 'input-available':\n                    return (\n                      <div key={callId}>\n                        Getting weather information for {part.input.city}...\n                      </div>\n                    );\n                  case 'output-available':\n                    return (\n                      <div key={callId}>\n                        Weather in {part.input.city}: {part.output}\n                      </div>\n                    );\n                  case 'output-error':\n                    return (\n                      <div key={callId}>\n                        Error getting weather for {part.input.city}:{' '}\n                        {part.errorText}\n                      </div>\n                    );\n                }\n                break;\n              }\n            }\n          })}\n          <br />\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input value={input} onChange={e => setInput(e.target.value)} />\n      </form>\n    </>\n  );\n}\nError handling\n\nSometimes an error may occur during client-side tool execution. Use the addToolResult method with a state of output-error and errorText value instead of output record the error.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport {\n  DefaultChatTransport,\n  lastAssistantMessageIsCompleteWithToolCalls,\n} from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, addToolResult } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n\n\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n\n    // run client-side tools that are automatically executed:\n    async onToolCall({ toolCall }) {\n      // Check if it's a dynamic tool first for proper type narrowing\n      if (toolCall.dynamic) {\n        return;\n      }\n\n\n      if (toolCall.toolName === 'getWeatherInformation') {\n        try {\n          const weather = await getWeatherInformation(toolCall.input);\n\n\n          // No await - avoids potential deadlocks\n          addToolResult({\n            tool: 'getWeatherInformation',\n            toolCallId: toolCall.toolCallId,\n            output: weather,\n          });\n        } catch (err) {\n          addToolResult({\n            tool: 'getWeatherInformation',\n            toolCallId: toolCall.toolCallId,\n            state: 'output-error',\n            errorText: 'Unable to get the weather information',\n          });\n        }\n      }\n    },\n  });\n}\nDynamic Tools\n\nWhen using dynamic tools (tools with unknown types at compile time), the UI parts use a generic dynamic-tool type instead of specific tool types:\n\napp/page.tsx\n{\n  message.parts.map((part, index) => {\n    switch (part.type) {\n      // Static tools with specific (`tool-${toolName}`) types\n      case 'tool-getWeatherInformation':\n        return <WeatherDisplay part={part} />;\n\n\n      // Dynamic tools use generic `dynamic-tool` type\n      case 'dynamic-tool':\n        return (\n          <div key={index}>\n            <h4>Tool: {part.toolName}</h4>\n            {part.state === 'input-streaming' && (\n              <pre>{JSON.stringify(part.input, null, 2)}</pre>\n            )}\n            {part.state === 'output-available' && (\n              <pre>{JSON.stringify(part.output, null, 2)}</pre>\n            )}\n            {part.state === 'output-error' && (\n              <div>Error: {part.errorText}</div>\n            )}\n          </div>\n        );\n    }\n  });\n}\n\nDynamic tools are useful when integrating with:\n\nMCP (Model Context Protocol) tools without schemas\nUser-defined functions loaded at runtime\nExternal tool providers\nTool call streaming\n\nTool call streaming is enabled by default in AI SDK 5.0, allowing you to stream tool calls while they are being generated. This provides a better user experience by showing tool inputs as they are generated in real-time.\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    // toolCallStreaming is enabled by default in v5\n    // ...\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nWith tool call streaming enabled, partial tool calls are streamed as part of the data stream. They are available through the useChat hook. The typed tool parts of assistant messages will also contain partial tool calls. You can use the state property of the tool part to render the correct UI.\n\napp/page.tsx\nexport default function Chat() {\n  // ...\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          {message.parts.map(part => {\n            switch (part.type) {\n              case 'tool-askForConfirmation':\n              case 'tool-getLocation':\n              case 'tool-getWeatherInformation':\n                switch (part.state) {\n                  case 'input-streaming':\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n                  case 'input-available':\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n                  case 'output-available':\n                    return <pre>{JSON.stringify(part.output, null, 2)}</pre>;\n                  case 'output-error':\n                    return <div>Error: {part.errorText}</div>;\n                }\n            }\n          })}\n        </div>\n      ))}\n    </>\n  );\n}\nStep start parts\n\nWhen you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages. If you want to display boundaries between tool calls, you can use the step-start parts as follows:\n\napp/page.tsx\n// ...\n// where you render the message parts:\nmessage.parts.map((part, index) => {\n  switch (part.type) {\n    case 'step-start':\n      // show step boundaries as horizontal lines:\n      return index > 0 ? (\n        <div key={index} className=\"text-gray-500\">\n          <hr className=\"my-2 border-gray-300\" />\n        </div>\n      ) : null;\n    case 'text':\n    // ...\n    case 'tool-askForConfirmation':\n    case 'tool-getLocation':\n    case 'tool-getWeatherInformation':\n    // ...\n  }\n});\n// ...\nServer-side Multi-Step Calls\n\nYou can also use multi-step calls on the server-side with streamText. This works when all invoked tools have an execute function on the server side.\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage, stepCountIs } from 'ai';\nimport { z } from 'zod';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        inputSchema: z.object({ city: z.string() }),\n        // tool has execute function:\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n    },\n    stopWhen: stepCountIs(5),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nErrors\n\nLanguage models can make errors when calling tools. By default, these errors are masked for security reasons, and show up as \"An error occurred\" in the UI.\n\nTo surface the errors, you can use the onError function when calling toUIMessageResponse.\n\nexport function errorHandler(error: unknown) {\n  if (error == null) {\n    return 'unknown error';\n  }\n\n\n  if (typeof error === 'string') {\n    return error;\n  }\n\n\n  if (error instanceof Error) {\n    return error.message;\n  }\n\n\n  return JSON.stringify(error);\n}\nconst result = streamText({\n  // ...\n});\n\n\nreturn result.toUIMessageStreamResponse({\n  onError: errorHandler,\n});\n\nIn case you are using createUIMessageResponse, you can use the onError function when calling toUIMessageResponse:\n\nconst response = createUIMessageResponse({\n  // ...\n  async execute(dataStream) {\n    // ...\n  },\n  onError: error => `Custom error: ${error.message}`,\n});\nPrevious\nChatbot Resume Streams\nNext\nGenerative User Interfaces"
  },
  {
    "title": "AI SDK UI: Generative User Interfaces",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces",
    "html": "AI SDK UI\nGenerative User Interfaces\nCopy markdown\nGenerative User Interfaces\n\nGenerative user interfaces (generative UI) is the process of allowing a large language model (LLM) to go beyond text and \"generate UI\". This creates a more engaging and AI-native experience for users.\n\nWhat is the weather in SF?\ngetWeather(\"San Francisco\")\nThursday, March 7\n47°\nSunny\n7am\n48°\n8am\n50°\n9am\n52°\n10am\n54°\n11am\n56°\n12pm\n58°\n1pm\n60°\nThanks!\n\nAt the core of generative UI are tools , which are functions you provide to the model to perform specialized tasks like getting the weather in a location. The model can decide when and how to use these tools based on the context of the conversation.\n\nGenerative UI is the process of connecting the results of a tool call to a React component. Here's how it works:\n\nYou provide the model with a prompt or conversation history, along with a set of tools.\nBased on the context, the model may decide to call a tool.\nIf a tool is called, it will execute and return data.\nThis data can then be passed to a React component for rendering.\n\nBy passing the tool results to React components, you can create a generative UI experience that's more engaging and adaptive to your needs.\n\nBuild a Generative UI Chat Interface\n\nLet's create a chat interface that handles text-based conversations and incorporates dynamic UI elements based on model responses.\n\nBasic Chat Implementation\n\nStart with a basic chat implementation using the useChat hook:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Page() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>\n          <div>\n            {message.parts.map((part, index) => {\n              if (part.type === 'text') {\n                return <span key={index}>{part.text}</span>;\n              }\n              return null;\n            })}\n          </div>\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n\nTo handle the chat requests and model responses, set up an API route:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';\n\n\nexport async function POST(request: Request) {\n  const { messages }: { messages: UIMessage[] } = await request.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a friendly assistant!',\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nThis API route uses the streamText function to process chat messages and stream the model's responses back to the client.\n\nCreate a Tool\n\nBefore enhancing your chat interface with dynamic UI elements, you need to create a tool and corresponding React component. A tool will allow the model to perform a specific action, such as fetching weather information.\n\nCreate a new file called ai/tools.ts with the following content:\n\nai/tools.ts\nimport { tool as createTool } from 'ai';\nimport { z } from 'zod';\n\n\nexport const weatherTool = createTool({\n  description: 'Display the weather for a location',\n  inputSchema: z.object({\n    location: z.string().describe('The location to get the weather for'),\n  }),\n  execute: async function ({ location }) {\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    return { weather: 'Sunny', temperature: 75, location };\n  },\n});\n\n\nexport const tools = {\n  displayWeather: weatherTool,\n};\n\nIn this file, you've created a tool called weatherTool. This tool simulates fetching weather information for a given location. This tool will return simulated data after a 2-second delay. In a real-world application, you would replace this simulation with an actual API call to a weather service.\n\nUpdate the API Route\n\nUpdate the API route to include the tool you've defined:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, convertToModelMessages, UIMessage, stepCountIs } from 'ai';\nimport { tools } from '@/ai/tools';\n\n\nexport async function POST(request: Request) {\n  const { messages }: { messages: UIMessage[] } = await request.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    system: 'You are a friendly assistant!',\n    messages: convertToModelMessages(messages),\n    stopWhen: stepCountIs(5),\n    tools,\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nNow that you've defined the tool and added it to your streamText call, let's build a React component to display the weather information it returns.\n\nCreate UI Components\n\nCreate a new file called components/weather.tsx:\n\ncomponents/weather.tsx\ntype WeatherProps = {\n  temperature: number;\n  weather: string;\n  location: string;\n};\n\n\nexport const Weather = ({ temperature, weather, location }: WeatherProps) => {\n  return (\n    <div>\n      <h2>Current Weather for {location}</h2>\n      <p>Condition: {weather}</p>\n      <p>Temperature: {temperature}°C</p>\n    </div>\n  );\n};\n\nThis component will display the weather information for a given location. It takes three props: temperature, weather, and location (exactly what the weatherTool returns).\n\nRender the Weather Component\n\nNow that you have your tool and corresponding React component, let's integrate them into your chat interface. You'll render the Weather component when the model calls the weather tool.\n\nTo check if the model has called a tool, you can check the parts array of the UIMessage object for tool-specific parts. In AI SDK 5.0, tool parts use typed naming: tool-${toolName} instead of generic types.\n\nUpdate your page.tsx file:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\nimport { Weather } from '@/components/weather';\n\n\nexport default function Page() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>\n          <div>\n            {message.parts.map((part, index) => {\n              if (part.type === 'text') {\n                return <span key={index}>{part.text}</span>;\n              }\n\n\n              if (part.type === 'tool-displayWeather') {\n                switch (part.state) {\n                  case 'input-available':\n                    return <div key={index}>Loading weather...</div>;\n                  case 'output-available':\n                    return (\n                      <div key={index}>\n                        <Weather {...part.output} />\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={index}>Error: {part.errorText}</div>;\n                  default:\n                    return null;\n                }\n              }\n\n\n              return null;\n            })}\n          </div>\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n\nIn this updated code snippet, you:\n\nUse manual input state management with useState instead of the built-in input and handleInputChange.\nUse sendMessage instead of handleSubmit to send messages.\nCheck the parts array of each message for different content types.\nHandle tool parts with type tool-displayWeather and their different states (input-available, output-available, output-error).\n\nThis approach allows you to dynamically render UI components based on the model's responses, creating a more interactive and context-aware chat experience.\n\nExpanding Your Generative UI Application\n\nYou can enhance your chat application by adding more tools and components, creating a richer and more versatile user experience. Here's how you can expand your application:\n\nAdding More Tools\n\nTo add more tools, simply define them in your ai/tools.ts file:\n\n// Add a new stock tool\nexport const stockTool = createTool({\n  description: 'Get price for a stock',\n  inputSchema: z.object({\n    symbol: z.string().describe('The stock symbol to get the price for'),\n  }),\n  execute: async function ({ symbol }) {\n    // Simulated API call\n    await new Promise(resolve => setTimeout(resolve, 2000));\n    return { symbol, price: 100 };\n  },\n});\n\n\n// Update the tools object\nexport const tools = {\n  displayWeather: weatherTool,\n  getStockPrice: stockTool,\n};\n\nNow, create a new file called components/stock.tsx:\n\ntype StockProps = {\n  price: number;\n  symbol: string;\n};\n\n\nexport const Stock = ({ price, symbol }: StockProps) => {\n  return (\n    <div>\n      <h2>Stock Information</h2>\n      <p>Symbol: {symbol}</p>\n      <p>Price: ${price}</p>\n    </div>\n  );\n};\n\nFinally, update your page.tsx file to include the new Stock component:\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\nimport { Weather } from '@/components/weather';\nimport { Stock } from '@/components/stock';\n\n\nexport default function Page() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>{message.role}</div>\n          <div>\n            {message.parts.map((part, index) => {\n              if (part.type === 'text') {\n                return <span key={index}>{part.text}</span>;\n              }\n\n\n              if (part.type === 'tool-displayWeather') {\n                switch (part.state) {\n                  case 'input-available':\n                    return <div key={index}>Loading weather...</div>;\n                  case 'output-available':\n                    return (\n                      <div key={index}>\n                        <Weather {...part.output} />\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={index}>Error: {part.errorText}</div>;\n                  default:\n                    return null;\n                }\n              }\n\n\n              if (part.type === 'tool-getStockPrice') {\n                switch (part.state) {\n                  case 'input-available':\n                    return <div key={index}>Loading stock price...</div>;\n                  case 'output-available':\n                    return (\n                      <div key={index}>\n                        <Stock {...part.output} />\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={index}>Error: {part.errorText}</div>;\n                  default:\n                    return null;\n                }\n              }\n\n\n              return null;\n            })}\n          </div>\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          type=\"text\"\n          value={input}\n          onChange={e => setInput(e.target.value)}\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </div>\n  );\n}\n\nBy following this pattern, you can continue to add more tools and components, expanding the capabilities of your Generative UI application.\n\nPrevious\nChatbot Tool Usage\nNext\nCompletion"
  },
  {
    "title": "AI SDK UI: Completion",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/completion",
    "html": "AI SDK UI\nCompletion\nCopy markdown\nCompletion\n\nThe useCompletion hook allows you to create a user interface to handle text completions in your application. It enables the streaming of text completions from your AI provider, manages the state for chat input, and updates the UI automatically as new messages are received.\n\nThe useCompletion hook is now part of the @ai-sdk/react package.\n\nIn this guide, you will learn how to use the useCompletion hook in your application to generate text completions and stream them in real-time to your users.\n\nExample\napp/page.tsx\n'use client';\n\n\nimport { useCompletion } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const { completion, input, handleInputChange, handleSubmit } = useCompletion({\n    api: '/api/completion',\n  });\n\n\n  return (\n    <form onSubmit={handleSubmit}>\n      <input\n        name=\"prompt\"\n        value={input}\n        onChange={handleInputChange}\n        id=\"input\"\n      />\n      <button type=\"submit\">Submit</button>\n      <div>{completion}</div>\n    </form>\n  );\n}\napp/api/completion/route.ts\nimport { streamText } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { prompt }: { prompt: string } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-3.5-turbo'),\n    prompt,\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nIn the Page component, the useCompletion hook will request to your AI provider endpoint whenever the user submits a message. The completion is then streamed back in real-time and displayed in the UI.\n\nThis enables a seamless text completion experience where the user can see the AI response as soon as it is available, without having to wait for the entire response to be received.\n\nCustomized UI\n\nuseCompletion also provides ways to manage the prompt via code, show loading and error states, and update messages without being triggered by user interactions.\n\nLoading and error states\n\nTo show a loading spinner while the chatbot is processing the user's message, you can use the isLoading state returned by the useCompletion hook:\n\nconst { isLoading, ... } = useCompletion()\n\n\nreturn(\n  <>\n    {isLoading ? <Spinner /> : null}\n  </>\n)\n\nSimilarly, the error state reflects the error object thrown during the fetch request. It can be used to display an error message, or show a toast notification:\n\nconst { error, ... } = useCompletion()\n\n\nuseEffect(() => {\n  if (error) {\n    toast.error(error.message)\n  }\n}, [error])\n\n\n// Or display the error message in the UI:\nreturn (\n  <>\n    {error ? <div>{error.message}</div> : null}\n  </>\n)\nControlled input\n\nIn the initial example, we have handleSubmit and handleInputChange callbacks that manage the input changes and form submissions. These are handy for common use cases, but you can also use uncontrolled APIs for more advanced scenarios such as form validation or customized components.\n\nThe following example demonstrates how to use more granular APIs like setInput with your custom input and submit button components:\n\nconst { input, setInput } = useCompletion();\n\n\nreturn (\n  <>\n    <MyCustomInput value={input} onChange={value => setInput(value)} />\n  </>\n);\nCancelation\n\nIt's also a common use case to abort the response message while it's still streaming back from the AI provider. You can do this by calling the stop function returned by the useCompletion hook.\n\nconst { stop, isLoading, ... } = useCompletion()\n\n\nreturn (\n  <>\n    <button onClick={stop} disabled={!isLoading}>Stop</button>\n  </>\n)\n\nWhen the user clicks the \"Stop\" button, the fetch request will be aborted. This avoids consuming unnecessary resources and improves the UX of your application.\n\nThrottling UI Updates\nThis feature is currently only available for React.\n\nBy default, the useCompletion hook will trigger a render every time a new chunk is received. You can throttle the UI updates with the experimental_throttle option.\n\npage.tsx\nconst { completion, ... } = useCompletion({\n  // Throttle the completion and data updates to 50ms:\n  experimental_throttle: 50\n})\nEvent Callbacks\n\nuseCompletion also provides optional event callbacks that you can use to handle different stages of the chatbot lifecycle. These callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\nconst { ... } = useCompletion({\n  onResponse: (response: Response) => {\n    console.log('Received response from server:', response)\n  },\n  onFinish: (prompt: string, completion: string) => {\n    console.log('Finished streaming completion:', completion)\n  },\n  onError: (error: Error) => {\n    console.error('An error occurred:', error)\n  },\n})\n\nIt's worth noting that you can abort the processing by throwing an error in the onResponse callback. This will trigger the onError callback and stop the message from being appended to the chat UI. This can be useful for handling unexpected responses from the AI provider.\n\nConfigure Request Options\n\nBy default, the useCompletion hook sends a HTTP POST request to the /api/completion endpoint with the prompt as part of the request body. You can customize the request by passing additional options to the useCompletion hook:\n\nconst { messages, input, handleInputChange, handleSubmit } = useCompletion({\n  api: '/api/custom-completion',\n  headers: {\n    Authorization: 'your_token',\n  },\n  body: {\n    user_id: '123',\n  },\n  credentials: 'same-origin',\n});\n\nIn this example, the useCompletion hook sends a POST request to the /api/completion endpoint with the specified headers, additional body fields, and credentials for that fetch request. On your server side, you can handle the request with these additional information.\n\nPrevious\nGenerative User Interfaces\nNext\nObject Generation"
  },
  {
    "title": "AI SDK UI: Object Generation",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/object-generation",
    "html": "AI SDK UI\nObject Generation\nCopy markdown\nObject Generation\n\nuseObject is an experimental feature and only available in React, Svelte, and Vue.\n\nThe useObject hook allows you to create interfaces that represent a structured JSON object that is being streamed.\n\nIn this guide, you will learn how to use the useObject hook in your application to generate UIs for structured data on the fly.\n\nExample\n\nThe example shows a small notifications demo app that generates fake notifications in real-time.\n\nSchema\n\nIt is helpful to set up the schema in a separate file that is imported on both the client and server.\n\napp/api/notifications/schema.ts\nimport { z } from 'zod';\n\n\n// define a schema for the notifications\nexport const notificationSchema = z.object({\n  notifications: z.array(\n    z.object({\n      name: z.string().describe('Name of a fictional person.'),\n      message: z.string().describe('Message. Do not use emojis or links.'),\n    }),\n  ),\n});\nClient\n\nThe client uses useObject to stream the object generation process.\n\nThe results are partial and are displayed as they are received. Please note the code for handling undefined values in the JSX.\n\napp/page.tsx\n'use client';\n\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/notifications/schema';\n\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n\n  return (\n    <>\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\nServer\n\nOn the server, we use streamObject to stream the object generation process.\n\napp/api/notifications/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { notificationSchema } from './schema';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n\n  const result = streamObject({\n    model: openai('gpt-4.1'),\n    schema: notificationSchema,\n    prompt:\n      `Generate 3 notifications for a messages app in this context:` + context,\n  });\n\n\n  return result.toTextStreamResponse();\n}\nEnum Output Mode\n\nWhen you need to classify or categorize input into predefined options, you can use the enum output mode with useObject. This requires a specific schema structure where the object has enum as a key with z.enum containing your possible values.\n\nExample: Text Classification\n\nThis example shows how to build a simple text classifier that categorizes statements as true or false.\n\nClient\n\nWhen using useObject with enum output mode, your schema must be an object with enum as the key:\n\napp/classify/page.tsx\n'use client';\n\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { z } from 'zod';\n\n\nexport default function ClassifyPage() {\n  const { object, submit, isLoading } = useObject({\n    api: '/api/classify',\n    schema: z.object({ enum: z.enum(['true', 'false']) }),\n  });\n\n\n  return (\n    <>\n      <button onClick={() => submit('The earth is flat')} disabled={isLoading}>\n        Classify statement\n      </button>\n\n\n      {object && <div>Classification: {object.enum}</div>}\n    </>\n  );\n}\nServer\n\nOn the server, use streamObject with output: 'enum' to stream the classification result:\n\napp/api/classify/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\n\n\nexport async function POST(req: Request) {\n  const context = await req.json();\n\n\n  const result = streamObject({\n    model: openai('gpt-4.1'),\n    output: 'enum',\n    enum: ['true', 'false'],\n    prompt: `Classify this statement as true or false: ${context}`,\n  });\n\n\n  return result.toTextStreamResponse();\n}\nCustomized UI\n\nuseObject also provides ways to show loading and error states:\n\nLoading State\n\nThe isLoading state returned by the useObject hook can be used for several purposes:\n\nTo show a loading spinner while the object is generated.\nTo disable the submit button.\napp/page.tsx\n'use client';\n\n\nimport { useObject } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const { isLoading, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n\n  return (\n    <>\n      {isLoading && <Spinner />}\n\n\n      <button\n        onClick={() => submit('Messages during finals week.')}\n        disabled={isLoading}\n      >\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\nStop Handler\n\nThe stop function can be used to stop the object generation process. This can be useful if the user wants to cancel the request or if the server is taking too long to respond.\n\napp/page.tsx\n'use client';\n\n\nimport { useObject } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const { isLoading, stop, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n\n  return (\n    <>\n      {isLoading && (\n        <button type=\"button\" onClick={() => stop()}>\n          Stop\n        </button>\n      )}\n\n\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\nError State\n\nSimilarly, the error state reflects the error object thrown during the fetch request. It can be used to display an error message, or to disable the submit button:\n\nWe recommend showing a generic error message to the user, such as \"Something went wrong.\" This is a good practice to avoid leaking information from the server.\n\n'use client';\n\n\nimport { useObject } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const { error, object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n  });\n\n\n  return (\n    <>\n      {error && <div>An error occurred.</div>}\n\n\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </>\n  );\n}\nEvent Callbacks\n\nuseObject provides optional event callbacks that you can use to handle life-cycle events.\n\nonFinish: Called when the object generation is completed.\nonError: Called when an error occurs during the fetch request.\n\nThese callbacks can be used to trigger additional actions, such as logging, analytics, or custom UI updates.\n\napp/page.tsx\n'use client';\n\n\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\nimport { notificationSchema } from './api/notifications/schema';\n\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/notifications',\n    schema: notificationSchema,\n    onFinish({ object, error }) {\n      // typed object, undefined if schema validation fails:\n      console.log('Object generation completed:', object);\n\n\n      // error, undefined if schema validation succeeds:\n      console.log('Schema validation error:', error);\n    },\n    onError(error) {\n      // error during fetch request:\n      console.error('An error occurred:', error);\n    },\n  });\n\n\n  return (\n    <div>\n      <button onClick={() => submit('Messages during finals week.')}>\n        Generate notifications\n      </button>\n\n\n      {object?.notifications?.map((notification, index) => (\n        <div key={index}>\n          <p>{notification?.name}</p>\n          <p>{notification?.message}</p>\n        </div>\n      ))}\n    </div>\n  );\n}\nConfigure Request Options\n\nYou can configure the API endpoint, optional headers and credentials using the api, headers and credentials settings.\n\nconst { submit, object } = useObject({\n  api: '/api/use-object',\n  headers: {\n    'X-Custom-Header': 'CustomValue',\n  },\n  credentials: 'include',\n  schema: yourSchema,\n});\nPrevious\nCompletion\nNext\nStreaming Custom Data"
  },
  {
    "title": "AI SDK UI: Streaming Custom Data",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/streaming-data",
    "html": "AI SDK UI\nStreaming Custom Data\nCopy markdown\nStreaming Custom Data\n\nIt is often useful to send additional data alongside the model's response. For example, you may want to send status information, the message ids after storing them, or references to content that the language model is referring to.\n\nThe AI SDK provides several helpers that allows you to stream additional data to the client and attach it to the UIMessage parts array:\n\ncreateUIMessageStream: creates a data stream\ncreateUIMessageStreamResponse: creates a response object that streams data\npipeUIMessageStreamToResponse: pipes a data stream to a server response object\n\nThe data is streamed as part of the response stream using Server-Sent Events.\n\nSetting Up Type-Safe Data Streaming\n\nFirst, define your custom message type with data part schemas for type safety:\n\nai/types.ts\nimport { UIMessage } from 'ai';\n\n\n// Define your custom message type with data part schemas\nexport type MyUIMessage = UIMessage<\n  never, // metadata type\n  {\n    weather: {\n      city: string;\n      weather?: string;\n      status: 'loading' | 'success';\n    };\n    notification: {\n      message: string;\n      level: 'info' | 'warning' | 'error';\n    };\n  } // data parts type\n>;\nStreaming Data from the Server\n\nIn your server-side route handler, you can create a UIMessageStream and then pass it to createUIMessageStreamResponse:\n\nroute.ts\nimport { openai } from '@ai-sdk/openai';\nimport {\n  createUIMessageStream,\n  createUIMessageStreamResponse,\n  streamText,\n  convertToModelMessages,\n} from 'ai';\nimport type { MyUIMessage } from '@/ai/types';\n\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n\n  const stream = createUIMessageStream<MyUIMessage>({\n    execute: ({ writer }) => {\n      // 1. Send initial status (transient - won't be added to message history)\n      writer.write({\n        type: 'data-notification',\n        data: { message: 'Processing your request...', level: 'info' },\n        transient: true, // This part won't be added to message history\n      });\n\n\n      // 2. Send sources (useful for RAG use cases)\n      writer.write({\n        type: 'source',\n        value: {\n          type: 'source',\n          sourceType: 'url',\n          id: 'source-1',\n          url: 'https://weather.com',\n          title: 'Weather Data Source',\n        },\n      });\n\n\n      // 3. Send data parts with loading state\n      writer.write({\n        type: 'data-weather',\n        id: 'weather-1',\n        data: { city: 'San Francisco', status: 'loading' },\n      });\n\n\n      const result = streamText({\n        model: openai('gpt-4.1'),\n        messages: convertToModelMessages(messages),\n        onFinish() {\n          // 4. Update the same data part (reconciliation)\n          writer.write({\n            type: 'data-weather',\n            id: 'weather-1', // Same ID = update existing part\n            data: {\n              city: 'San Francisco',\n              weather: 'sunny',\n              status: 'success',\n            },\n          });\n\n\n          // 5. Send completion notification (transient)\n          writer.write({\n            type: 'data-notification',\n            data: { message: 'Request completed', level: 'info' },\n            transient: true, // Won't be added to message history\n          });\n        },\n      });\n\n\n      writer.merge(result.toUIMessageStream());\n    },\n  });\n\n\n  return createUIMessageStreamResponse({ stream });\n}\n\nYou can also send stream data from custom backends, e.g. Python / FastAPI, using the UI Message Stream Protocol.\n\nTypes of Streamable Data\nData Parts (Persistent)\n\nRegular data parts are added to the message history and appear in message.parts:\n\nwriter.write({\n  type: 'data-weather',\n  id: 'weather-1', // Optional: enables reconciliation\n  data: { city: 'San Francisco', status: 'loading' },\n});\nSources\n\nSources are useful for RAG implementations where you want to show which documents or URLs were referenced:\n\nwriter.write({\n  type: 'source',\n  value: {\n    type: 'source',\n    sourceType: 'url',\n    id: 'source-1',\n    url: 'https://example.com',\n    title: 'Example Source',\n  },\n});\nTransient Data Parts (Ephemeral)\n\nTransient parts are sent to the client but not added to the message history. They are only accessible via the onData useChat handler:\n\n// server\nwriter.write({\n  type: 'data-notification',\n  data: { message: 'Processing...', level: 'info' },\n  transient: true, // Won't be added to message history\n});\n\n\n// client\nconst [notification, setNotification] = useState();\n\n\nconst { messages } = useChat({\n  onData: ({ data, type }) => {\n    if (type === 'data-notification') {\n      setNotification({ message: data.message, level: data.level });\n    }\n  },\n});\nData Part Reconciliation\n\nWhen you write to a data part with the same ID, the client automatically reconciles and updates that part. This enables powerful dynamic experiences like:\n\nCollaborative artifacts - Update code, documents, or designs in real-time\nProgressive data loading - Show loading states that transform into final results\nLive status updates - Update progress bars, counters, or status indicators\nInteractive components - Build UI elements that evolve based on user interaction\n\nThe reconciliation happens automatically - simply use the same id when writing to the stream.\n\nProcessing Data on the Client\nUsing the onData Callback\n\nThe onData callback is essential for handling streaming data, especially transient parts:\n\npage.tsx\nimport { useChat } from '@ai-sdk/react';\nimport type { MyUIMessage } from '@/ai/types';\n\n\nconst { messages } = useChat<MyUIMessage>({\n  api: '/api/chat',\n  onData: dataPart => {\n    // Handle all data parts as they arrive (including transient parts)\n    console.log('Received data part:', dataPart);\n\n\n    // Handle different data part types\n    if (dataPart.type === 'data-weather') {\n      console.log('Weather update:', dataPart.data);\n    }\n\n\n    // Handle transient notifications (ONLY available here, not in message.parts)\n    if (dataPart.type === 'data-notification') {\n      showToast(dataPart.data.message, dataPart.data.level);\n    }\n  },\n});\n\nImportant: Transient data parts are only available through the onData callback. They will not appear in the message.parts array since they're not added to message history.\n\nRendering Persistent Data Parts\n\nYou can filter and render data parts from the message parts array:\n\npage.tsx\nconst result = (\n  <>\n    {messages?.map(message => (\n      <div key={message.id}>\n        {/* Render weather data parts */}\n        {message.parts\n          .filter(part => part.type === 'data-weather')\n          .map((part, index) => (\n            <div key={index} className=\"weather-widget\">\n              {part.data.status === 'loading' ? (\n                <>Getting weather for {part.data.city}...</>\n              ) : (\n                <>\n                  Weather in {part.data.city}: {part.data.weather}\n                </>\n              )}\n            </div>\n          ))}\n\n\n        {/* Render text content */}\n        {message.parts\n          .filter(part => part.type === 'text')\n          .map((part, index) => (\n            <div key={index}>{part.text}</div>\n          ))}\n\n\n        {/* Render sources */}\n        {message.parts\n          .filter(part => part.type === 'source')\n          .map((part, index) => (\n            <div key={index} className=\"source\">\n              Source: <a href={part.url}>{part.title}</a>\n            </div>\n          ))}\n      </div>\n    ))}\n  </>\n);\nComplete Example\npage.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\nimport type { MyUIMessage } from '@/ai/types';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n\n\n  const { messages, sendMessage } = useChat<MyUIMessage>({\n    api: '/api/chat',\n    onData: dataPart => {\n      // Handle transient notifications\n      if (dataPart.type === 'data-notification') {\n        console.log('Notification:', dataPart.data.message);\n      }\n    },\n  });\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n\n\n          {/* Render weather data */}\n          {message.parts\n            .filter(part => part.type === 'data-weather')\n            .map((part, index) => (\n              <span key={index} className=\"weather-update\">\n                {part.data.status === 'loading' ? (\n                  <>Getting weather for {part.data.city}...</>\n                ) : (\n                  <>\n                    Weather in {part.data.city}: {part.data.weather}\n                  </>\n                )}\n              </span>\n            ))}\n\n\n          {/* Render text content */}\n          {message.parts\n            .filter(part => part.type === 'text')\n            .map((part, index) => (\n              <div key={index}>{part.text}</div>\n            ))}\n        </div>\n      ))}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          placeholder=\"Ask about the weather...\"\n        />\n        <button type=\"submit\">Send</button>\n      </form>\n    </>\n  );\n}\nUse Cases\nRAG Applications - Stream sources and retrieved documents\nReal-time Status - Show loading states and progress updates\nCollaborative Tools - Stream live updates to shared artifacts\nAnalytics - Send usage data without cluttering message history\nNotifications - Display temporary alerts and status messages\nMessage Metadata vs Data Parts\n\nBoth message metadata and data parts allow you to send additional information alongside messages, but they serve different purposes:\n\nMessage Metadata\n\nMessage metadata is best for message-level information that describes the message as a whole:\n\nAttached at the message level via message.metadata\nSent using the messageMetadata callback in toUIMessageStreamResponse\nIdeal for: timestamps, model info, token usage, user context\nType-safe with custom metadata types\n// Server: Send metadata about the message\nreturn result.toUIMessageStreamResponse({\n  messageMetadata: ({ part }) => {\n    if (part.type === 'finish') {\n      return {\n        model: part.response.modelId,\n        totalTokens: part.totalUsage.totalTokens,\n        createdAt: Date.now(),\n      };\n    }\n  },\n});\nData Parts\n\nData parts are best for streaming dynamic arbitrary data:\n\nAdded to the message parts array via message.parts\nStreamed using createUIMessageStream and writer.write()\nCan be reconciled/updated using the same ID\nSupport transient parts that don't persist\nIdeal for: dynamic content, loading states, interactive components\n// Server: Stream data as part of message content\nwriter.write({\n  type: 'data-weather',\n  id: 'weather-1',\n  data: { city: 'San Francisco', status: 'loading' },\n});\n\nFor more details on message metadata, see the Message Metadata documentation.\n\nPrevious\nObject Generation\nNext\nError Handling"
  },
  {
    "title": "AI SDK UI: Error Handling",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/error-handling",
    "html": "AI SDK UI\nError Handling\nCopy markdown\nError Handling and warnings\nWarnings\n\nThe AI SDK shows warnings when something might not work as expected. These warnings help you fix problems before they cause errors.\n\nWhen Warnings Appear\n\nWarnings are shown in the browser console when:\n\nUnsupported settings: You use a setting that the AI model doesn't support\nUnsupported tools: You use a tool that the AI model can't use\nOther issues: The AI model reports other problems\nWarning Messages\n\nAll warnings start with \"AI SDK Warning:\" so you can easily find them. For example:\n\nAI SDK Warning: The \"temperature\" setting is not supported by this model\nAI SDK Warning: The tool \"calculator\" is not supported by this model\nTurning Off Warnings\n\nBy default, warnings are shown in the console. You can control this behavior:\n\nTurn Off All Warnings\n\nSet a global variable to turn off warnings completely:\n\nglobalThis.AI_SDK_LOG_WARNINGS = false;\nCustom Warning Handler\n\nYou can also provide your own function to handle warnings:\n\nglobalThis.AI_SDK_LOG_WARNINGS = warnings => {\n  // Handle warnings your own way\n  warnings.forEach(warning => {\n    // Your custom logic here\n    console.log('Custom warning:', warning);\n  });\n};\n\nCustom warning functions are experimental and can change in patch releases without notice.\n\nError Handling\nError Helper Object\n\nEach AI SDK UI hook also returns an error object that you can use to render the error in your UI. You can use the error object to show an error message, disable the submit button, or show a retry button.\n\nWe recommend showing a generic error message to the user, such as \"Something went wrong.\" This is a good practice to avoid leaking information from the server.\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage, error, regenerate } = useChat();\n\n\n  const handleSubmit = (e: React.FormEvent) => {\n    e.preventDefault();\n    sendMessage({ text: input });\n    setInput('');\n  };\n\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}:{' '}\n          {m.parts\n            .filter(part => part.type === 'text')\n            .map(part => part.text)\n            .join('')}\n        </div>\n      ))}\n\n\n      {error && (\n        <>\n          <div>An error occurred.</div>\n          <button type=\"button\" onClick={() => regenerate()}>\n            Retry\n          </button>\n        </>\n      )}\n\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={e => setInput(e.target.value)}\n          disabled={error != null}\n        />\n      </form>\n    </div>\n  );\n}\nAlternative: replace last message\n\nAlternatively you can write a custom submit handler that replaces the last message when an error is present.\n\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { sendMessage, error, messages, setMessages } = useChat();\n\n\n  function customSubmit(event: React.FormEvent<HTMLFormElement>) {\n    event.preventDefault();\n\n\n    if (error != null) {\n      setMessages(messages.slice(0, -1)); // remove last message\n    }\n\n\n    sendMessage({ text: input });\n    setInput('');\n  }\n\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}:{' '}\n          {m.parts\n            .filter(part => part.type === 'text')\n            .map(part => part.text)\n            .join('')}\n        </div>\n      ))}\n\n\n      {error && <div>An error occurred.</div>}\n\n\n      <form onSubmit={customSubmit}>\n        <input value={input} onChange={e => setInput(e.target.value)} />\n      </form>\n    </div>\n  );\n}\nError Handling Callback\n\nErrors can be processed by passing an onError callback function as an option to the useChat or useCompletion hooks. The callback function receives an error object as an argument.\n\nimport { useChat } from '@ai-sdk/react';\n\n\nexport default function Page() {\n  const {\n    /* ... */\n  } = useChat({\n    // handle error:\n    onError: error => {\n      console.error(error);\n    },\n  });\n}\nInjecting Errors for Testing\n\nYou might want to create errors for testing. You can easily do so by throwing an error in your route handler:\n\nexport async function POST(req: Request) {\n  throw new Error('This is a test error');\n}\nPrevious\nStreaming Custom Data\nNext\nTransport"
  },
  {
    "title": "AI SDK UI: Transport",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/transport",
    "html": "AI SDK UI\nTransport\nCopy markdown\nTransport\n\nThe useChat transport system provides fine-grained control over how messages are sent to your API endpoints and how responses are processed. This is particularly useful for alternative communication protocols like WebSockets, custom authentication patterns, or specialized backend integrations.\n\nDefault Transport\n\nBy default, useChat uses HTTP POST requests to send messages to /api/chat:\n\nimport { useChat } from '@ai-sdk/react';\n\n\n// Uses default HTTP transport\nconst { messages, sendMessage } = useChat();\n\nThis is equivalent to:\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/chat',\n  }),\n});\nCustom Transport Configuration\n\nConfigure the default transport with custom options:\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\n\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/custom-chat',\n    headers: {\n      Authorization: 'Bearer your-token',\n      'X-API-Version': '2024-01',\n    },\n    credentials: 'include',\n  }),\n});\nDynamic Configuration\n\nYou can also provide functions that return configuration values. This is useful for authentication tokens that need to be refreshed, or for configuration that depends on runtime conditions:\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/chat',\n    headers: () => ({\n      Authorization: `Bearer ${getAuthToken()}`,\n      'X-User-ID': getCurrentUserId(),\n    }),\n    body: () => ({\n      sessionId: getCurrentSessionId(),\n      preferences: getUserPreferences(),\n    }),\n    credentials: () => 'include',\n  }),\n});\nRequest Transformation\n\nTransform requests before sending to your API:\n\nconst { messages, sendMessage } = useChat({\n  transport: new DefaultChatTransport({\n    api: '/api/chat',\n    prepareSendMessagesRequest: ({ id, messages, trigger, messageId }) => {\n      return {\n        headers: {\n          'X-Session-ID': id,\n        },\n        body: {\n          messages: messages.slice(-10), // Only send last 10 messages\n          trigger,\n          messageId,\n        },\n      };\n    },\n  }),\n});\nBuilding Custom Transports\n\nTo understand how to build your own transport, refer to the source code of the default implementation:\n\nDefaultChatTransport\n - The complete default HTTP transport implementation\nHttpChatTransport\n - Base HTTP transport with request handling\nChatTransport Interface\n - The transport interface you need to implement\n\nThese implementations show you exactly how to:\n\nHandle the sendMessages method\nProcess UI message streams\nTransform requests and responses\nHandle errors and connection management\n\nThe transport system gives you complete control over how your chat application communicates, enabling integration with any backend protocol or service.\n\nPrevious\nError Handling\nNext\nReading UIMessage Streams"
  },
  {
    "title": "AI SDK UI: Reading UIMessage Streams",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/reading-ui-message-streams",
    "html": "AI SDK UI\nReading UIMessage Streams\nCopy markdown\nReading UI Message Streams\n\nUIMessage streams are useful outside of traditional chat use cases. You can consume them for terminal UIs, custom stream processing on the client, or React Server Components (RSC).\n\nThe readUIMessageStream helper transforms a stream of UIMessageChunk objects into an AsyncIterableStream of UIMessage objects, allowing you to process messages as they're being constructed.\n\nBasic Usage\nimport { openai } from '@ai-sdk/openai';\nimport { readUIMessageStream, streamText } from 'ai';\n\n\nasync function main() {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    prompt: 'Write a short story about a robot.',\n  });\n\n\n  for await (const uiMessage of readUIMessageStream({\n    stream: result.toUIMessageStream(),\n  })) {\n    console.log('Current message state:', uiMessage);\n  }\n}\nTool Calls Integration\n\nHandle streaming responses that include tool calls:\n\nimport { openai } from '@ai-sdk/openai';\nimport { readUIMessageStream, streamText, tool } from 'ai';\nimport { z } from 'zod';\n\n\nasync function handleToolCalls() {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    tools: {\n      weather: tool({\n        description: 'Get the weather in a location',\n        inputSchema: z.object({\n          location: z.string().describe('The location to get the weather for'),\n        }),\n        execute: ({ location }) => ({\n          location,\n          temperature: 72 + Math.floor(Math.random() * 21) - 10,\n        }),\n      }),\n    },\n    prompt: 'What is the weather in Tokyo?',\n  });\n\n\n  for await (const uiMessage of readUIMessageStream({\n    stream: result.toUIMessageStream(),\n  })) {\n    // Handle different part types\n    uiMessage.parts.forEach(part => {\n      switch (part.type) {\n        case 'text':\n          console.log('Text:', part.text);\n          break;\n        case 'tool-call':\n          console.log('Tool called:', part.toolName, 'with args:', part.args);\n          break;\n        case 'tool-result':\n          console.log('Tool result:', part.result);\n          break;\n      }\n    });\n  }\n}\nResuming Conversations\n\nResume streaming from a previous message state:\n\nimport { readUIMessageStream, streamText } from 'ai';\n\n\nasync function resumeConversation(lastMessage: UIMessage) {\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: [\n      { role: 'user', content: 'Continue our previous conversation.' },\n    ],\n  });\n\n\n  // Resume from the last message\n  for await (const uiMessage of readUIMessageStream({\n    stream: result.toUIMessageStream(),\n    message: lastMessage, // Resume from this message\n  })) {\n    console.log('Resumed message:', uiMessage);\n  }\n}\nPrevious\nTransport\nNext\nMessage Metadata"
  },
  {
    "title": "AI SDK UI: Message Metadata",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/message-metadata",
    "html": "AI SDK UI\nMessage Metadata\nCopy markdown\nMessage Metadata\n\nMessage metadata allows you to attach custom information to messages at the message level. This is useful for tracking timestamps, model information, token usage, user context, and other message-level data.\n\nOverview\n\nMessage metadata differs from data parts in that it's attached at the message level rather than being part of the message content. While data parts are ideal for dynamic content that forms part of the message, metadata is perfect for information about the message itself.\n\nGetting Started\n\nHere's a simple example of using message metadata to track timestamps and model information:\n\nDefining Metadata Types\n\nFirst, define your metadata type for type safety:\n\napp/types.ts\nimport { UIMessage } from 'ai';\nimport { z } from 'zod';\n\n\n// Define your metadata schema\nexport const messageMetadataSchema = z.object({\n  createdAt: z.number().optional(),\n  model: z.string().optional(),\n  totalTokens: z.number().optional(),\n});\n\n\nexport type MessageMetadata = z.infer<typeof messageMetadataSchema>;\n\n\n// Create a typed UIMessage\nexport type MyUIMessage = UIMessage<MessageMetadata>;\nSending Metadata from the Server\n\nUse the messageMetadata callback in toUIMessageStreamResponse to send metadata at different streaming stages:\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText } from 'ai';\nimport type { MyUIMessage } from '@/types';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: MyUIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse({\n    originalMessages: messages, // pass this in for type-safe return objects\n    messageMetadata: ({ part }) => {\n      // Send metadata when streaming starts\n      if (part.type === 'start') {\n        return {\n          createdAt: Date.now(),\n          model: 'gpt-4o',\n        };\n      }\n\n\n      // Send additional metadata when streaming completes\n      if (part.type === 'finish') {\n        return {\n          totalTokens: part.totalUsage.totalTokens,\n        };\n      }\n    },\n  });\n}\n\nTo enable type-safe metadata return object in messageMetadata, pass in the originalMessages parameter typed to your UIMessage type.\n\nAccessing Metadata on the Client\n\nAccess metadata through the message.metadata property:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { DefaultChatTransport } from 'ai';\nimport type { MyUIMessage } from '@/types';\n\n\nexport default function Chat() {\n  const { messages } = useChat<MyUIMessage>({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n  });\n\n\n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <div>\n            {message.role === 'user' ? 'User: ' : 'AI: '}\n            {message.metadata?.createdAt && (\n              <span className=\"text-sm text-gray-500\">\n                {new Date(message.metadata.createdAt).toLocaleTimeString()}\n              </span>\n            )}\n          </div>\n\n\n          {/* Render message content */}\n          {message.parts.map((part, index) =>\n            part.type === 'text' ? <div key={index}>{part.text}</div> : null,\n          )}\n\n\n          {/* Display additional metadata */}\n          {message.metadata?.totalTokens && (\n            <div className=\"text-xs text-gray-400\">\n              {message.metadata.totalTokens} tokens\n            </div>\n          )}\n        </div>\n      ))}\n    </div>\n  );\n}\n\nFor streaming arbitrary data that changes during generation, consider using data parts instead.\n\nCommon Use Cases\n\nMessage metadata is ideal for:\n\nTimestamps: When messages were created or completed\nModel Information: Which AI model was used\nToken Usage: Track costs and usage limits\nUser Context: User IDs, session information\nPerformance Metrics: Generation time, time to first token\nQuality Indicators: Finish reason, confidence scores\nSee Also\nChatbot Guide - Message metadata in the context of building chatbots\nStreaming Data - Comparison with data parts\nUIMessage Reference - Complete UIMessage type reference\nPrevious\nReading UIMessage Streams\nNext\nStream Protocols"
  },
  {
    "title": "AI SDK UI: Stream Protocols",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol",
    "html": "AI SDK UI\nStream Protocols\nCopy markdown\nStream Protocols\n\nAI SDK UI functions such as useChat and useCompletion support both text streams and data streams. The stream protocol defines how the data is streamed to the frontend on top of the HTTP protocol.\n\nThis page describes both protocols and how to use them in the backend and frontend.\n\nYou can use this information to develop custom backends and frontends for your use case, e.g., to provide compatible API endpoints that are implemented in a different language such as Python.\n\nFor instance, here's an example using FastAPI\n as a backend.\n\nText Stream Protocol\n\nA text stream contains chunks in plain text, that are streamed to the frontend. Each chunk is then appended together to form a full text response.\n\nText streams are supported by useChat, useCompletion, and useObject. When you use useChat or useCompletion, you need to enable text streaming by setting the streamProtocol options to text.\n\nYou can generate text streams with streamText in the backend. When you call toTextStreamResponse() on the result object, a streaming HTTP response is returned.\n\nText streams only support basic text data. If you need to stream other types of data such as tool calls, use data streams.\n\nText Stream Example\n\nHere is a Next.js example that uses the text stream protocol:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { TextStreamChatTransport } from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat({\n    transport: new TextStreamChatTransport({ api: '/api/chat' }),\n  });\n\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\napp/api/chat/route.ts\nimport { streamText, UIMessage, convertToModelMessages } from 'ai';\nimport { openai } from '@ai-sdk/openai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toTextStreamResponse();\n}\nData Stream Protocol\n\nA data stream follows a special protocol that the AI SDK provides to send information to the frontend.\n\nThe data stream protocol uses Server-Sent Events (SSE) format for improved standardization, keep-alive through ping, reconnect capabilities, and better cache handling.\n\nWhen you provide data streams from a custom backend, you need to set the x-vercel-ai-ui-message-stream header to v1.\n\nThe following stream parts are currently supported:\n\nMessage Start Part\n\nIndicates the beginning of a new message with metadata.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"start\",\"messageId\":\"...\"}\nText Parts\n\nText content is streamed using a start/delta/end pattern with unique IDs for each text block.\n\nText Start Part\n\nIndicates the beginning of a text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"text-start\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\nText Delta Part\n\nContains incremental text content for the text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"text-delta\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\",\"delta\":\"Hello\"}\nText End Part\n\nIndicates the completion of a text block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"text-end\",\"id\":\"msg_68679a454370819ca74c8eb3d04379630dd1afb72306ca5d\"}\nReasoning Parts\n\nReasoning content is streamed using a start/delta/end pattern with unique IDs for each reasoning block.\n\nReasoning Start Part\n\nIndicates the beginning of a reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"reasoning-start\",\"id\":\"reasoning_123\"}\nReasoning Delta Part\n\nContains incremental reasoning content for the reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"reasoning-delta\",\"id\":\"reasoning_123\",\"delta\":\"This is some reasoning\"}\nReasoning End Part\n\nIndicates the completion of a reasoning block.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"reasoning-end\",\"id\":\"reasoning_123\"}\nSource Parts\n\nSource parts provide references to external content sources.\n\nSource URL Part\n\nReferences to external URLs.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"source-url\",\"sourceId\":\"https://example.com\",\"url\":\"https://example.com\"}\nSource Document Part\n\nReferences to documents or files.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"source-document\",\"sourceId\":\"https://example.com\",\"mediaType\":\"file\",\"title\":\"Title\"}\nFile Part\n\nThe file parts contain references to files with their media type.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"file\",\"url\":\"https://example.com/file.png\",\"mediaType\":\"image/png\"}\nData Parts\n\nCustom data parts allow streaming of arbitrary structured data with type-specific handling.\n\nFormat: Server-Sent Event with JSON object where the type includes a custom suffix\n\nExample:\n\ndata: {\"type\":\"data-weather\",\"data\":{\"location\":\"SF\",\"temperature\":100}}\n\nThe data-* type pattern allows you to define custom data types that your frontend can handle specifically.\n\nError Part\n\nThe error parts are appended to the message as they are received.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"error\",\"errorText\":\"error message\"}\nTool Input Start Part\n\nIndicates the beginning of tool input streaming.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"tool-input-start\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\"}\nTool Input Delta Part\n\nIncremental chunks of tool input as it's being generated.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"tool-input-delta\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"inputTextDelta\":\"San Francisco\"}\nTool Input Available Part\n\nIndicates that tool input is complete and ready for execution.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"tool-input-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"toolName\":\"getWeatherInformation\",\"input\":{\"city\":\"San Francisco\"}}\nTool Output Available Part\n\nContains the result of tool execution.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"tool-output-available\",\"toolCallId\":\"call_fJdQDqnXeGxTmr4E3YPSR7Ar\",\"output\":{\"city\":\"San Francisco\",\"weather\":\"sunny\"}}\nStart Step Part\n\nA part indicating the start of a step.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"start-step\"}\nFinish Step Part\n\nA part indicating that a step (i.e., one LLM API call in the backend) has been completed.\n\nThis part is necessary to correctly process multiple stitched assistant calls, e.g. when calling tools in the backend, and using steps in useChat at the same time.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"finish-step\"}\nFinish Message Part\n\nA part indicating the completion of a message.\n\nFormat: Server-Sent Event with JSON object\n\nExample:\n\ndata: {\"type\":\"finish\"}\nStream Termination\n\nThe stream ends with a special [DONE] marker.\n\nFormat: Server-Sent Event with literal [DONE]\n\nExample:\n\ndata: [DONE]\n\nThe data stream protocol is supported by useChat and useCompletion on the frontend and used by default. useCompletion only supports the text and data stream parts.\n\nOn the backend, you can use toUIMessageStreamResponse() from the streamText result object to return a streaming HTTP response.\n\nUI Message Stream Example\n\nHere is a Next.js example that uses the UI message stream protocol:\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const [input, setInput] = useState('');\n  const { messages, sendMessage } = useChat();\n\n\n  return (\n    <div className=\"flex flex-col w-full max-w-md py-24 mx-auto stretch\">\n      {messages.map(message => (\n        <div key={message.id} className=\"whitespace-pre-wrap\">\n          {message.role === 'user' ? 'User: ' : 'AI: '}\n          {message.parts.map((part, i) => {\n            switch (part.type) {\n              case 'text':\n                return <div key={`${message.id}-${i}`}>{part.text}</div>;\n            }\n          })}\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          sendMessage({ text: input });\n          setInput('');\n        }}\n      >\n        <input\n          className=\"fixed dark:bg-zinc-900 bottom-0 w-full max-w-md p-2 mb-8 border border-zinc-300 dark:border-zinc-800 rounded shadow-xl\"\n          value={input}\n          placeholder=\"Say something...\"\n          onChange={e => setInput(e.currentTarget.value)}\n        />\n      </form>\n    </div>\n  );\n}\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, UIMessage, convertToModelMessages } from 'ai';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nPrevious\nMessage Metadata\nNext\nAI SDK RSC"
  },
  {
    "title": "AI SDK UI: Chatbot Tool Usage",
    "url": "https://ai-sdk.dev/docs/ai-sdk-ui/chatbot-tool-usage",
    "html": "AI SDK UI\nChatbot Tool Usage\nCopy markdown\nChatbot Tool Usage\n\nWith useChat and streamText, you can use tools in your chatbot application. The AI SDK supports three types of tools in this context:\n\nAutomatically executed server-side tools\nAutomatically executed client-side tools\nTools that require user interaction, such as confirmation dialogs\n\nThe flow is as follows:\n\nThe user enters a message in the chat UI.\nThe message is sent to the API route.\nIn your server side route, the language model generates tool calls during the streamText call.\nAll tool calls are forwarded to the client.\nServer-side tools are executed using their execute method and their results are forwarded to the client.\nClient-side tools that should be automatically executed are handled with the onToolCall callback. You must call addToolResult to provide the tool result.\nClient-side tool that require user interactions can be displayed in the UI. The tool calls and results are available as tool invocation parts in the parts property of the last assistant message.\nWhen the user interaction is done, addToolResult can be used to add the tool result to the chat.\nThe chat can be configured to automatically submit when all tool results are available using sendAutomaticallyWhen. This triggers another iteration of this flow.\n\nThe tool calls and tool executions are integrated into the assistant message as typed tool parts. A tool part is at first a tool call, and then it becomes a tool result when the tool is executed. The tool result contains all information about the tool call as well as the result of the tool execution.\n\nTool result submission can be configured using the sendAutomaticallyWhen option. You can use the lastAssistantMessageIsCompleteWithToolCalls helper to automatically submit when all tool results are available. This simplifies the client-side code while still allowing full control when needed.\n\nExample\n\nIn this example, we'll use three tools:\n\ngetWeatherInformation: An automatically executed server-side tool that returns the weather in a given city.\naskForConfirmation: A user-interaction client-side tool that asks the user for confirmation.\ngetLocation: An automatically executed client-side tool that returns a random city.\nAPI route\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage } from 'ai';\nimport { z } from 'zod';\n\n\n// Allow streaming responses up to 30 seconds\nexport const maxDuration = 30;\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      // server-side tool with execute function:\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        inputSchema: z.object({ city: z.string() }),\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n      // client-side tool that starts user interaction:\n      askForConfirmation: {\n        description: 'Ask the user for confirmation.',\n        inputSchema: z.object({\n          message: z.string().describe('The message to ask for confirmation.'),\n        }),\n      },\n      // client-side tool that is automatically executed on the client:\n      getLocation: {\n        description:\n          'Get the user location. Always ask for confirmation before using this tool.',\n        inputSchema: z.object({}),\n      },\n    },\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nClient-side page\n\nThe client-side page uses the useChat hook to create a chatbot application with real-time message streaming. Tool calls are displayed in the chat UI as typed tool parts. Please make sure to render the messages using the parts property of the message.\n\nThere are three things worth mentioning:\n\nThe onToolCall callback is used to handle client-side tools that should be automatically executed. In this example, the getLocation tool is a client-side tool that returns a random city. You call addToolResult to provide the result (without await to avoid potential deadlocks).\n\nAlways check if (toolCall.dynamic) first in your onToolCall handler. Without this check, TypeScript will throw an error like: Type 'string' is not assignable to type '\"toolName1\" | \"toolName2\"' when you try to use toolCall.toolName in addToolResult.\n\nThe sendAutomaticallyWhen option with lastAssistantMessageIsCompleteWithToolCalls helper automatically submits when all tool results are available.\n\nThe parts array of assistant messages contains tool parts with typed names like tool-askForConfirmation. The client-side tool askForConfirmation is displayed in the UI. It asks the user for confirmation and displays the result once the user confirms or denies the execution. The result is added to the chat using addToolResult with the tool parameter for type safety.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport {\n  DefaultChatTransport,\n  lastAssistantMessageIsCompleteWithToolCalls,\n} from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, addToolResult } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n\n\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n\n    // run client-side tools that are automatically executed:\n    async onToolCall({ toolCall }) {\n      // Check if it's a dynamic tool first for proper type narrowing\n      if (toolCall.dynamic) {\n        return;\n      }\n\n\n      if (toolCall.toolName === 'getLocation') {\n        const cities = ['New York', 'Los Angeles', 'Chicago', 'San Francisco'];\n\n\n        // No await - avoids potential deadlocks\n        addToolResult({\n          tool: 'getLocation',\n          toolCallId: toolCall.toolCallId,\n          output: cities[Math.floor(Math.random() * cities.length)],\n        });\n      }\n    },\n  });\n  const [input, setInput] = useState('');\n\n\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          <strong>{`${message.role}: `}</strong>\n          {message.parts.map(part => {\n            switch (part.type) {\n              // render text parts as simple text:\n              case 'text':\n                return part.text;\n\n\n              // for tool parts, use the typed tool part names:\n              case 'tool-askForConfirmation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  case 'input-streaming':\n                    return (\n                      <div key={callId}>Loading confirmation request...</div>\n                    );\n                  case 'input-available':\n                    return (\n                      <div key={callId}>\n                        {part.input.message}\n                        <div>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                tool: 'askForConfirmation',\n                                toolCallId: callId,\n                                output: 'Yes, confirmed.',\n                              })\n                            }\n                          >\n                            Yes\n                          </button>\n                          <button\n                            onClick={() =>\n                              addToolResult({\n                                tool: 'askForConfirmation',\n                                toolCallId: callId,\n                                output: 'No, denied',\n                              })\n                            }\n                          >\n                            No\n                          </button>\n                        </div>\n                      </div>\n                    );\n                  case 'output-available':\n                    return (\n                      <div key={callId}>\n                        Location access allowed: {part.output}\n                      </div>\n                    );\n                  case 'output-error':\n                    return <div key={callId}>Error: {part.errorText}</div>;\n                }\n                break;\n              }\n\n\n              case 'tool-getLocation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  case 'input-streaming':\n                    return (\n                      <div key={callId}>Preparing location request...</div>\n                    );\n                  case 'input-available':\n                    return <div key={callId}>Getting location...</div>;\n                  case 'output-available':\n                    return <div key={callId}>Location: {part.output}</div>;\n                  case 'output-error':\n                    return (\n                      <div key={callId}>\n                        Error getting location: {part.errorText}\n                      </div>\n                    );\n                }\n                break;\n              }\n\n\n              case 'tool-getWeatherInformation': {\n                const callId = part.toolCallId;\n\n\n                switch (part.state) {\n                  // example of pre-rendering streaming tool inputs:\n                  case 'input-streaming':\n                    return (\n                      <pre key={callId}>{JSON.stringify(part, null, 2)}</pre>\n                    );\n                  case 'input-available':\n                    return (\n                      <div key={callId}>\n                        Getting weather information for {part.input.city}...\n                      </div>\n                    );\n                  case 'output-available':\n                    return (\n                      <div key={callId}>\n                        Weather in {part.input.city}: {part.output}\n                      </div>\n                    );\n                  case 'output-error':\n                    return (\n                      <div key={callId}>\n                        Error getting weather for {part.input.city}:{' '}\n                        {part.errorText}\n                      </div>\n                    );\n                }\n                break;\n              }\n            }\n          })}\n          <br />\n        </div>\n      ))}\n\n\n      <form\n        onSubmit={e => {\n          e.preventDefault();\n          if (input.trim()) {\n            sendMessage({ text: input });\n            setInput('');\n          }\n        }}\n      >\n        <input value={input} onChange={e => setInput(e.target.value)} />\n      </form>\n    </>\n  );\n}\nError handling\n\nSometimes an error may occur during client-side tool execution. Use the addToolResult method with a state of output-error and errorText value instead of output record the error.\n\napp/page.tsx\n'use client';\n\n\nimport { useChat } from '@ai-sdk/react';\nimport {\n  DefaultChatTransport,\n  lastAssistantMessageIsCompleteWithToolCalls,\n} from 'ai';\nimport { useState } from 'react';\n\n\nexport default function Chat() {\n  const { messages, sendMessage, addToolResult } = useChat({\n    transport: new DefaultChatTransport({\n      api: '/api/chat',\n    }),\n\n\n    sendAutomaticallyWhen: lastAssistantMessageIsCompleteWithToolCalls,\n\n\n    // run client-side tools that are automatically executed:\n    async onToolCall({ toolCall }) {\n      // Check if it's a dynamic tool first for proper type narrowing\n      if (toolCall.dynamic) {\n        return;\n      }\n\n\n      if (toolCall.toolName === 'getWeatherInformation') {\n        try {\n          const weather = await getWeatherInformation(toolCall.input);\n\n\n          // No await - avoids potential deadlocks\n          addToolResult({\n            tool: 'getWeatherInformation',\n            toolCallId: toolCall.toolCallId,\n            output: weather,\n          });\n        } catch (err) {\n          addToolResult({\n            tool: 'getWeatherInformation',\n            toolCallId: toolCall.toolCallId,\n            state: 'output-error',\n            errorText: 'Unable to get the weather information',\n          });\n        }\n      }\n    },\n  });\n}\nDynamic Tools\n\nWhen using dynamic tools (tools with unknown types at compile time), the UI parts use a generic dynamic-tool type instead of specific tool types:\n\napp/page.tsx\n{\n  message.parts.map((part, index) => {\n    switch (part.type) {\n      // Static tools with specific (`tool-${toolName}`) types\n      case 'tool-getWeatherInformation':\n        return <WeatherDisplay part={part} />;\n\n\n      // Dynamic tools use generic `dynamic-tool` type\n      case 'dynamic-tool':\n        return (\n          <div key={index}>\n            <h4>Tool: {part.toolName}</h4>\n            {part.state === 'input-streaming' && (\n              <pre>{JSON.stringify(part.input, null, 2)}</pre>\n            )}\n            {part.state === 'output-available' && (\n              <pre>{JSON.stringify(part.output, null, 2)}</pre>\n            )}\n            {part.state === 'output-error' && (\n              <div>Error: {part.errorText}</div>\n            )}\n          </div>\n        );\n    }\n  });\n}\n\nDynamic tools are useful when integrating with:\n\nMCP (Model Context Protocol) tools without schemas\nUser-defined functions loaded at runtime\nExternal tool providers\nTool call streaming\n\nTool call streaming is enabled by default in AI SDK 5.0, allowing you to stream tool calls while they are being generated. This provides a better user experience by showing tool inputs as they are generated in real-time.\n\napp/api/chat/route.ts\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    // toolCallStreaming is enabled by default in v5\n    // ...\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\n\nWith tool call streaming enabled, partial tool calls are streamed as part of the data stream. They are available through the useChat hook. The typed tool parts of assistant messages will also contain partial tool calls. You can use the state property of the tool part to render the correct UI.\n\napp/page.tsx\nexport default function Chat() {\n  // ...\n  return (\n    <>\n      {messages?.map(message => (\n        <div key={message.id}>\n          {message.parts.map(part => {\n            switch (part.type) {\n              case 'tool-askForConfirmation':\n              case 'tool-getLocation':\n              case 'tool-getWeatherInformation':\n                switch (part.state) {\n                  case 'input-streaming':\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n                  case 'input-available':\n                    return <pre>{JSON.stringify(part.input, null, 2)}</pre>;\n                  case 'output-available':\n                    return <pre>{JSON.stringify(part.output, null, 2)}</pre>;\n                  case 'output-error':\n                    return <div>Error: {part.errorText}</div>;\n                }\n            }\n          })}\n        </div>\n      ))}\n    </>\n  );\n}\nStep start parts\n\nWhen you are using multi-step tool calls, the AI SDK will add step start parts to the assistant messages. If you want to display boundaries between tool calls, you can use the step-start parts as follows:\n\napp/page.tsx\n// ...\n// where you render the message parts:\nmessage.parts.map((part, index) => {\n  switch (part.type) {\n    case 'step-start':\n      // show step boundaries as horizontal lines:\n      return index > 0 ? (\n        <div key={index} className=\"text-gray-500\">\n          <hr className=\"my-2 border-gray-300\" />\n        </div>\n      ) : null;\n    case 'text':\n    // ...\n    case 'tool-askForConfirmation':\n    case 'tool-getLocation':\n    case 'tool-getWeatherInformation':\n    // ...\n  }\n});\n// ...\nServer-side Multi-Step Calls\n\nYou can also use multi-step calls on the server-side with streamText. This works when all invoked tools have an execute function on the server side.\n\napp/api/chat/route.ts\nimport { openai } from '@ai-sdk/openai';\nimport { convertToModelMessages, streamText, UIMessage, stepCountIs } from 'ai';\nimport { z } from 'zod';\n\n\nexport async function POST(req: Request) {\n  const { messages }: { messages: UIMessage[] } = await req.json();\n\n\n  const result = streamText({\n    model: openai('gpt-4o'),\n    messages: convertToModelMessages(messages),\n    tools: {\n      getWeatherInformation: {\n        description: 'show the weather in a given city to the user',\n        inputSchema: z.object({ city: z.string() }),\n        // tool has execute function:\n        execute: async ({}: { city: string }) => {\n          const weatherOptions = ['sunny', 'cloudy', 'rainy', 'snowy', 'windy'];\n          return weatherOptions[\n            Math.floor(Math.random() * weatherOptions.length)\n          ];\n        },\n      },\n    },\n    stopWhen: stepCountIs(5),\n  });\n\n\n  return result.toUIMessageStreamResponse();\n}\nErrors\n\nLanguage models can make errors when calling tools. By default, these errors are masked for security reasons, and show up as \"An error occurred\" in the UI.\n\nTo surface the errors, you can use the onError function when calling toUIMessageResponse.\n\nexport function errorHandler(error: unknown) {\n  if (error == null) {\n    return 'unknown error';\n  }\n\n\n  if (typeof error === 'string') {\n    return error;\n  }\n\n\n  if (error instanceof Error) {\n    return error.message;\n  }\n\n\n  return JSON.stringify(error);\n}\nconst result = streamText({\n  // ...\n});\n\n\nreturn result.toUIMessageStreamResponse({\n  onError: errorHandler,\n});\n\nIn case you are using createUIMessageResponse, you can use the onError function when calling toUIMessageResponse:\n\nconst response = createUIMessageResponse({\n  // ...\n  async execute(dataStream) {\n    // ...\n  },\n  onError: error => `Custom error: ${error.message}`,\n});\nPrevious\nChatbot Resume Streams\nNext\nGenerative User Interfaces"
  }
]